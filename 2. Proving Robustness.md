# Chapter 2. Proving Neural Network Robustness


## Neural Network Robustness as a Verification Property

In this chapter we will learn about the problem that has received significanrt attention within the machine learning community: 
_the problem of robustness of neural networks to out-of-distribution shifts, also known as "robustness to adversarial attacks"._ 
The problem was famously raised by Christian Szegedy and his co-authors in 2013 in the paper ["Intriguing properties of neural networks"](<https://arxiv.org/pdf/1312.6199.pdf>)

So, here is the problem. Suppose we are given a data set $\mathcal{X}$ for classification of images, it consists of 
pairs $(\mathbf{x}, \mathbf{y})$, where $\mathbf{x} \in \mathbb{R}^n$ is an input, and  $\mathbf{y} \in \mathbb{R}^m$ is the desired output.
It is assumed that the outputs $\mathbf{y}$ are generated from $\mathbf{x}$ by some function $\mathcal{H} : \mathbb{R}^n → \mathbb{R}^m$ 
and that $\mathbf{x}$ is drawn from some probability distribution over $\mathbb{R}^n$. 

Let us take as an example the famous MNIST data set be LeCun et al. The images look like this:

![Images from the MNIST data set](images/MNIST.jpeg)

As we have already discussed in the previous chapter, a neural network is a function $f : \mathbb{R}^n → \mathbb{R}^m$  parametrised by 
a set of weights $\mathbf{w}$. 
The goal of training is to use the dataset $\mathcal{X}$ to find weights $\mathbf{w}$ such that $f$ approximates $\mathcal{H}$ well 
over input regions with high probability density.


When we train a neural network to be highly accurate on both the training and the test sets, we emprically test: 
 *  how well the neural network can in principle approximate  $\mathcal{H}$  (we do this by measuring its accuracy on the training set);
 *  how well that learnt hypothesis generalises to yet unseen data (we do this by measuring the accuracy on the test set).

 Coming to our example, if my neural network has a $99$ % accuracy on the MNIST data set, I should be satisfied that it learnt
 what a hand-written digit is. Szegedy et al were the first to show systematically that this is not the case:
take the image on the left (below), which is classified with high confidence as "0", apply perturbation on the middle,
and your neural network will give a $94$ % confidence that it sees a "5" on the right, even despite the fact that the image did 
not change the class (for the human eye):


 Original MNIST Image |    Perturbation        |  Resulting Perturbed Image
:-------------------------:|:-------------------------:|:-----------------------------
![Original MNIST Image](images/true.png) | ![Perturbation](images/eta.png) |  ![Perturbed Image](images/adv.png)

This experiment can be replicated for any data set and any neural network, no matter how accurate it is.

The root of the problem is: the image on the right no longer belongs to the probability distribution that the network has learnt (whether or not the image looks the same to a human observer).
We could phrase it differently: an ideal probability distribiution $\mathcal{H}$ that is "learnt" by a "human" accounts not only for the images we obtained as part of the dataset $\mathcal{X}$, but also involves an implicit assumption that "semantically" similar images belong to the same class.    

 The simplest way to capture this implicit assumption is to formulate a _verification property_ that insists that all similar images (images within an $\epsilon$ distance of each other in $\mathbb{R}^n$) are classified similarly. This property is often called $\epsilon$-ball robusness: For every image in the dataset, we assume we can "draw" a small $\epsilon$-ball around it, and guarantee that within that ball classification of the network does not change much (the ball's radius below is given by the chosen $\epsilon$):

 $\epsilon$-ball around a number "7" in MNIST |
 :-------------------------:|
|![epsilon-ball](images/neighbourhood-robustness.png)|

Formally, we define an $\epsilon$-ball  around an image $\hat{\mathbf{x}}$ as: 

$$\mathbb{B}(\hat{\mathbf{x}}, \epsilon) = [  \mathbf{x} \in \mathbb{R}^n: |\hat{\mathbf{x}}-\mathbf{x}| \leq \epsilon ]$$

where $| ... |$ is a distance function (or $L$-norm) in $\mathbb{R}^n$, such as Euclidean distance or $L_{\infty}$-norm. 


It now remains to define the property "classification of $f$ does not change much". The paper by [Casadio et al.](https://arxiv.org/abs/2104.01396)
summarises a few options for this definition. The simplest is the _"standard robustness"_ that requires that $|f(\hat{\mathbf{x}}) -  f(\mathbf{x})| \leq \delta$, for some small $\delta$.
We now assemble the desired _standard robustness_ property definition: 

Given an $\hat{\mathbf{x}} \in \mathcal{X}$,

$\forall \mathbf{x}. |\hat{\mathbf{x}}-\mathbf{x}| \leq \epsilon  \Longrightarrow |f(\hat{\mathbf{x}}) -  f(\mathbf{x})| \leq \delta $.


We refer the interested reader for a more detailed discussion of different robustness properties in: 
 * Marco Casadio, Ekaterina Komendantskaya, Matthew L. Daggitt, Wen Kokke, Guy Katz, Guy Amir, Idan Refaeli:
Neural Network Robustness as a Verification Property: A Principled Case Study. CAV (1) 2022: 219-231.

* Xiaowei Huang, Daniel Kroening, Wenjie Ruan, James Sharp, Youcheng Sun, Emese Thamo, Min Wu, Xinping Yi. A Survey of Safety and Trustworthiness of Deep Neural Networks: Verification, Testing, Adversarial Attack and Defence, and Interpretability. J. of Computer Science Review, 2018. 

## Formalising $\epsilon$-ball robustness for MNIST networks in Vehicle

We note that $\epsilon$-ball robustness as a verification property bears some similarity to the ACAS Xu example that we have already covered in Chapter 1. In particular, both verification properties impose constraints on the output regions of the neural networks, assuming some constraint on their input regions. (Both problems are therefore amenable to a range of interval propagation and abstract interpretation methods, see [this survey](https://arxiv.org/abs/1812.08342) for further details.) From the point of view of the property specification, which is our main concern here, there are two main differences between these two examples:

 * ACAS Xu did not have to refer to a dataset $\mathcal{X}$; $\epsilon$-ball robustness, however, is formulated relative to the samples present in the dataset.   
 
 * MNIST, as almost any other data set used in Computer Vision, has images represented as 2D arrays. Such data sets often require Convolutional Neural Networks (NN) that are best desgned to deal with 2D and 3D data.  

### 2D Arrays in vehicle

Starting a specification for MNIST data set follows the same pattern as we have seen in Chapter 1, only this time we declare inputs as 2D arrays:

``` vehicle
type Image = Tensor Rat [28, 28]
type Label = Index 10
```

As before, we define valid inputs, this time making a mild adaptation to 2D arrays and assuming all pixel values are normalised between 0 and 1:

``` vehicle
validImage : Image -> Bool
validImage x = forall i j . 0 <= x ! i ! j <= 1
```

The output of the network is a
-- score for each of the digits 0 to 9.

``` vehicle
@network
classifier : Image -> Vector Rat 10
```

We note again the use of the syntax for `@network`, marking the place where Vehicle interacts with an external tool (this time most likely with Python Tensorflow).

The classifier advises that input image `x` has label `i` if the score
for label `i` is greater than the score of any other label `j`.

``` vehicle
advises : Image -> Label -> Bool
advises x i = forall j . j != i => classifier x ! i > classifier x ! j
``` 

