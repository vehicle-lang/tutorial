\documentclass{beamer}

\usetheme{default}
\usecolortheme{rose}
\usefonttheme{serif}
%\usefonttheme{structureitalicsserif}

\definecolor{verdeuni}{rgb}{0.7,0.73437,0.55856}
\setbeamertemplate{headline}[verdeuni]
%\setbeamercovered{highly dinamic}
%\usepackage{eso-pic}

\usepackage{amsfonts,amsmath,amssymb,amsthm}
\usepackage[all]{xy}
\usepackage{array,url}
\usepackage{textcomp,textgreek}
\usepackage{pgfplots}
\usepackage{float}
\pgfplotsset{width=5cm,compat=1.9}
\usepackage{mdframed,wrapfig,subcaption}
%\usepackage[font=footnotesize,labelfont=it
%\usepackage[latin1]{inputenc}
\usepackage{babel}
\usepackage{color}
%\usepackage{url}
\usepackage{hyperref}
\usepackage{fancyvrb}
%\usepackage{tikz}
\usepackage{alltt}
%\usepackage{etex, xy}
%\usepackage{cibeamer}
\usepackage{tikz}
\usetikzlibrary{arrows,shapes}
%\xyoption{all} 
%\usepackage{listings}
%\input macro
\usepackage{cancel, comment}
\usepackage{verbatim}
\usepackage{slashbox}
\usepackage{ulem}

\newcommand{\tikzmark}[1]{\tikz[remember picture] \node[coordinate] (#1) {#1};}
\newcommand{\semitransp}[2][35]{\color{fg!#1}#2}

\usepackage{inconsolata,listings,listings-fstar}
\lstset{%
  language=FStar%
, style=colouredFStar%
, xleftmargin=1em%
, aboveskip=\smallskipamount%
, belowskip=\smallskipamount%
, basicstyle=\ttfamily\small%
, breaklines=true%
, breakatwhitespace=true%
, breakindent=0pt%
, escapeinside={(*@}{@*)}}

\usepackage[absolute,overlay]{textpos}
\beamertemplatenavigationsymbolsempty
\usepackage{ijcnn-diagram}

\usepackage[all]{foreign}

\newcommand{\fstar}{F$^\ast$\xspace}
\newcommand{\starchild}{StarChild\xspace}
\newcommand{\lazuli}{Lazuli\xspace}
\newcommand{\sapphire}{Sapphire\xspace}
\newcommand{\cL}{{\cal L}}
\newcommand{\Real}{{\mathbb R}}
\usepackage{makecell}
\DeclareMathOperator{\linear}{linear}
\DeclareMathOperator{\relu}{relu}
\DeclareMathOperator{\sigmoid}{sigmoid}
\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator{\neuron}{neuron}
\DeclareMathOperator{\truthy}{truthy}
\DeclareMathOperator{\falsey}{falsey}
\DeclareMathOperator{\neurontest}{test}
\usepackage{ellipsis}
\renewcommand{\ellipsisgap}{-0.25em}

\usepackage{soul}
\makeatletter
\let\HL\hl
\renewcommand\hl{%
  \let\set@color\beamerorig@set@color
  \let\reset@color\beamerorig@reset@color
  \HL}
\makeatother

%\usetikzlibrary{decorations.pathreplacing,shapes.arrows}
\newcommand\BackgroundPicture[1]{%
  \setbeamertemplate{background}{%
   \parbox[c][\paperheight]{\paperwidth}{%
      \vfill \hfill
\includegraphics[width=1\paperwidth,height=1\paperheight]{#1}
        \hfill \vfill
     }}}
\usepackage{xcolor,colortbl}
%\usepackage{listings}
\definecolor{Gray}{gray}{0.85}

%\setbeamertemplate{footline}[frame number]

%\newcommand{\rrdc}{\mbox{\,\(\Rightarrow\hspace{-9pt}\Rightarrow\)\,}} % Right reduction
%\newcommand{\lrdc}{\mbox{\,\(\Leftarrow\hspace{-9pt}\Leftarrow\)\,}}% Left reduction
%\newcommand{\lrrdc}{\mbox{\,\(\Leftarrow\hspace{-9pt}\Leftarrow\hspace{-5pt}\Rightarrow\hspace{-9pt}\Rightarrow\)\,}} % Equivalence
%\DeclareMathOperator{\id}{Id}
%\newcommand{\Zset}{\mathbb{Z}}
%\newcommand{\Bset}{\mathbb{Z}_2}

\setbeamertemplate{navigation symbols}{}

\mode<presentation>
\title[Continuous Verification of Machine Learning]{Continuous Verification of Machine Learning}
\subtitle{a Declarative Programming Approach}
\author[Ekaterina Komendantskaya]{Ekaterina Komendantskaya, \\ joint work with Daniel Kienitz and Wen Kokke
 }
\institute[www.LAIV.uk]{Lab for AI and Verification, Heriot-Watt University, Scotland}

\date[PPDP 2020]{Invited talk at PPDP 2020}

%\logo{\includegraphics[width=6cm]{LaivLogolong.png}}

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection]
  \end{frame}
}

\begin{document}
\BackgroundPicture{logo1.png}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{}


\begin{center}

\includegraphics[width=6cm]{Images/question.jpeg}

\end{center}
\end{frame}


\begin{frame}
\frametitle{Outline} \tableofcontents[pausesections]
\end{frame}

\section{Verification of AI: Overview and Motivation}

\begin{frame}
  \frametitle{Pervasive AI...}

  
 \begin{columns}
\column{.4\textwidth}
  
  \uncover<2->{\begin{block}{Autonomous cars}
     \begin{center}  \includegraphics[scale=.20]{Images/acar.jpeg}  \end{center} 
  \end{block}}

  
   \uncover<4->{
  \begin{block}{Robotics}
   \begin{center}  \includegraphics[scale=.10]{Images/nurse-robot-italy.jpg}  \end{center} 
 \end{block}}

\column{.4\textwidth}

\uncover<3->{\begin{block}{Smart Homes}
      \begin{center}  \includegraphics[scale=.20]{Images/smarthome.jpeg}  \end{center} 
  \end{block}}

   \uncover<5->{
    \begin{block}{Chat Bots}
     \begin{center}  \includegraphics[scale=.20]{Images/chatbot.jpeg}  \end{center} 
   \end{block}}

  \end{columns}

  
   \uncover<6>{
    \begin{block}{}
      \begin{center} ...and many more ...
      %  : from finance market bots to personal devices like insullin controllers
     % \end{center}
   % \end{block}}
  
  % \uncover<6>{
  %  \begin{block}{}
  %    \begin{center}
        AI is in urgent need of verification: safety, security, robustness to changing conditions and adversarial attacks, ...
				\end{center}
    \end{block}}

  \end{frame}



\begin{frame}
\frametitle{Lab for AI and Verification}

%\begin{block}{}

%\end{block}

\begin{itemize}
\item LAIV launched in March 2019
    \item ...in order to accumulate local expertise in AI, programming languages, verification
    \item ... and respond to demand
      in Edinburgh Robotarium and Edinburgh Center for Robotics

%	\item At the moment, 22 members: 7 academic staff, 3 RAs,
%          4 PhD and 8 MSc students not counting a number of collaborators. 
\end{itemize}

\begin{center}

\includegraphics[width=6cm]{LaivLogolong.png}

\end{center}
\end{frame}

\begin{frame}
  \frametitle{LAIV members: }
  
%\begin{center}
%  \includegraphics[width=7cm]{DSC_2635.jpg}
%\end{center}
 \begin{columns}
   \column{.2\textwidth}

   \includegraphics[width=1.8cm]{Images/Katya3.jpg}
      \begin{block}{}
     \end{block}
    \includegraphics[width=1.5cm]{Images/RS.jpg}
   \begin{block}{}
     \end{block}
    \includegraphics[width=1.5cm]{Images/air12.jpg}
  %\includegraphics[width=2cm]{Manuel_Maarek.jpg}

  % \includegraphics[width=2cm]{photoDP.jpg}
   \column{.2\textwidth}
   \includegraphics[width=1.3cm]{Images/liliaPhoto.jpg}
   \begin{block}{}
     \end{block}
  \includegraphics[width=1.5cm]{Images/Wei.jpeg}
  \column{.2\textwidth}
  \includegraphics[width=1.3cm]{Images/Hans.jpg}
     \begin{block}{}
     \end{block}
  \includegraphics[width=1.7cm]{Images/Ali.png}
        \begin{block}{}
     \end{block}
      \includegraphics[width=1.5cm]{Images/Wenjun.jpg}
      % \includegraphics[width=2cm]{Manuel_Maarek.jpg}
   \column{.2\textwidth}
      \includegraphics[width=1.5cm]{Images/Marco.png}
         % \includegraphics[width=2cm]{photoDP.jpg}
          \begin{block}{}
     \end{block}
  \includegraphics[width=1.5cm]{Images/Fraser.jpeg}
   \column{.2\textwidth}
   \includegraphics[width=1.5cm]{Images/Kirsty.jpg}
      \begin{block}{}
     \end{block}
     \includegraphics[width=1.5cm]{Images/MichaelLones.jpg}
        \begin{block}{}
     \end{block}
    \includegraphics[width=1.7cm]{Images/wenb.png}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Perception and Reasoning}

  \begin{center} AI methods divide into:
    \end{center}
  
   \begin{columns}
\column{.4\textwidth}

\uncover<2->{Perception tasks:}

  \uncover<2->{\begin{block}{Computer Vision}
     \begin{center}  \includegraphics[scale=.20]{Images/cv.jpeg}  \end{center} 
  \end{block}}

  
   \uncover<2->{
  \begin{block}{Natural language understanding}
   \begin{center}  \includegraphics[scale=.20]{Images/nlu.jpeg}  \end{center} 
 \end{block}}

\column{.4\textwidth}

\uncover<3->{Reasoning tasks:}

\uncover<3->{\begin{block}{Planning}
      \begin{center}  \includegraphics[scale=.20]{Images/route.jpeg}  \end{center} 
  \end{block}}

   \uncover<3->{
    \begin{block}{(Logical) reasoning}
     \begin{center}  \includegraphics[scale=.20]{Images/chatbot.jpeg}  \end{center} 
   \end{block}}

\end{columns}

  \uncover<3->{
    {\scriptsize
 \begin{thebibliography}{99}
 \beamertemplatearticlebibitems
\bibitem{1}{A.Hill, E.K. and R.Petrick: Proof-Carrying Plans: a Resource Logic for AI Planning. PPDP'20.}
 \end{thebibliography}}}
  
\end{frame}



  %\subsection{Why is it important?}

\begin{frame}
  \frametitle{Neural Networks...}
 

    
 \begin{columns}
   \column{.4\textwidth}

   
    \begin{center}
      \includegraphics[scale=0.3]{Images/NN}
    \end{center}  

\begin{block}{take care of \alert{\textbf{perception}} tasks:}

 \begin{itemize}
  \item[] computer vision 
  \item[] speech recognition
  \item[] pattern recognition
    \item[] ...
    \end{itemize}

  
\end{block}



\pause

\column{.4\textwidth}

 
\begin{block}{In:}
 
\begin{itemize}
\item[] autonomous cars
\item[] robots
%\item security applications
%\item financial applications
  \item[] medical applications
\item[] chatbots
%\item Google bot on mobile phones
\item[]  mobile phone apps 
\item[] $\ldots$
\end{itemize}

  \end{block}


 \end{columns}
    
\end{frame}

% \subsection{What is it?}

\section{Why Verifying  Neural Networks?}

\begin{frame}
  \frametitle{Neural network is}

  \begin{block}{... a function }
    $$N: \Real^n \rightarrow \Real^m$$
   % where $n$ is the size (or \emph{dimension}) of inputs, and $m$ -- the number of \emph{classes}. 
  \end{block}

  %By abuse of terminology, a \emph{training} algorithm that computes this function exactly (via a loss minimisation algorithm such as \emph{gradient decent})
  %is also often called a \emph{neural network}.

%  \pause
  
%  \begin{center}
%Ignore the learning functions for now...
    % \small{We will ignore the training algorithm for now, and will look at neural networks as functions. }
%\end{center}
  
\end{frame}


\begin{frame}
  \frametitle{Neural network is}
  %\begin{block}{}
   ... a function that separate inputs (data points) into classes
  %\end{block}
   \pause
   
  \begin{alertblock}{Suppose we have four data points}
       \begin{center}\begin{tabular}{l|ll c |}
      \hline
     & $x_1$ & $x_2$ & y  \\ \hline
    1 & 1 & 1 & 1  \\
    2 &  1 & 0 & 0 \\
    3 &  0 & 1 & 0 \\
    4 & 0 & 0 & 0 \\
      \hline
                     \end{tabular}
                     \end{center}
\end{alertblock}

\pause

  \begin{block}{We may look for a \alert{linear} function:}
 $$
\begin{array}{l}
  \neuron : (x_1:\Real) \to (x_2: \Real) \to (y: \Real)\\
  \neuron \; x_1 \; x_2 = b + w_{x_1} \times x_1 + w_{x_2} \times x_2
\end{array}
$$
  \end{block}


\end{frame}

\begin{frame}
  \frametitle{}
 

  \begin{alertblock}{Plotting these four data points in 3-dimensional space:}
    \begin{center}
        \begin{tikzpicture}
      \begin{axis}[
        xlabel={$x_1$},
        ylabel={$x_2$},
        zlabel={$y$},
        xmin=0, xmax=1,
        ymin=0, ymax=1,
        zmin=0, zmax=1,
        xtick={0,1},
        ytick={0,1},
        ztick={0,1},
        legend pos=north west,
        ymajorgrids=false,
        grid style=dashed,
        ]
        \addplot3[
        only marks,
        color=blue,
        scatter,
        mark=halfcircle*,
        mark size=2.9pt
        ]
        coordinates {
          (0,0,0)(1,0,0)(0,1,0)(1,1,1)
        };
      %  \addplot3[
      %  mesh,
      %  samples=10,
      %  domain=0:1,
      %  ]
      %  {(0.5*x+0.5*y+0)};
      \end{axis}
    \end{tikzpicture}
    \end{center}
\end{alertblock}




\end{frame}



\begin{frame}
  \frametitle{Neural network is}
   \begin{alertblock}{... a separating linear function:}
    \begin{center}
       \begin{tikzpicture}
      \begin{axis}[
        xlabel={$x_1$},
        ylabel={$x_2$},
        zlabel={class},
        xmin=0, xmax=1,
        ymin=0, ymax=1,
        zmin=0, zmax=1,
        xtick={0,1},
        ytick={0,1},
        ztick={0,1},
        legend pos=north west,
        ymajorgrids=false,
        grid style=dashed,
        ]
        \addplot3[
        only marks,
        color=blue,
        scatter,
        mark=halfcircle*,
        mark size=2.9pt
        ]
        coordinates {
          (0,0,0)(1,0,0)(0,1,0)(1,1,1)
        };
        \addplot3[
        mesh,
        samples=10,
        domain=0:1,
        ]
        {(0.5*x+0.5*y+0)};
      \end{axis}
    \end{tikzpicture}
    \end{center}
\end{alertblock}


\end{frame}


\begin{frame}
  \frametitle{}

    \begin{block}{Taking}
 $$
\begin{array}{l}
  \neuron : (x_1:\Real) \to (x_2: \Real) \to (y: \Real)\\
  \neuron \; x_1 \; x_2 = \alert<2->{b} + \alert<2->{w_{x_1}} \times x_1 + \alert<2->{w_{x_2}} \times x_2
\end{array}
$$
\end{block}

   \begin{alertblock}{here is its neuron view:}
    \begin{center}

     $\xymatrix@R=0.5pc@C=0pc{
      &*\txt{$x_1$}\ar[rrrr]&&&&*\txt{\alert<2->{$w_{x_1}$}}\ar[rrrrd]&&&&&&&&\\
      &*\txt{$x_2$}\ar[rrrr]&&&&*\txt{\alert<2->{$w_{x_2}$}}\ar[rrrr]&&&&*+++[o][F]{\alert<2->{+b}}\ar[rrr]&&&*\txt{ $y$ }&\\
    } $
  \end{center}
  \end{alertblock}
    
\end{frame}

\begin{frame}
  \frametitle{}

    \begin{block}{After running the training algorithm:}
 $$
\begin{array}{l}
  \neuron : (x_1:\Real) \to (x_2: \Real) \to (y: \Real)\\
  \neuron \; x_1 \; x_2 = \alert{-0.9} + \alert{0.5} \times x_1 + \alert{0.5} \times x_2
\end{array}
$$
\end{block}

   \begin{alertblock}{}
    \begin{center}

     $\xymatrix@R=0.5pc@C=0pc{
      &*\txt{$x_1$}\ar[rrrr]&&&&*\txt{\alert{$0.5$}}\ar[rrrrd]&&&&&&&&\\
      &*\txt{$x_2$}\ar[rrrr]&&&&*\txt{\alert{$0.5$}}\ar[rrrr]&&&&*+++[o][F]{\alert{-0.9}}\ar[rrr]&&&*\txt{ $y$ }&\\
    } $
  \end{center}
\end{alertblock}

(This is one of infinitely many solutions)
    
\end{frame}

\begin{frame}
  \frametitle{Neural networks}

    \begin{block}{Or may be we want to constrain the outputs:}
$$
\begin{array}{l}
  \neuron : (x_1:\Real) \to (x_2: \Real) \to (y: \Real\; \alert<2->{\{y=0 \lor y=1\}})\\
  \neuron \; x_1 \; x_2 = \alert<3->{S} \; (-0.9 + 0.5 x_1 + 0.5 x_2)
\end{array}
$$
\end{block}
\uncover<3->
{   \begin{alertblock}{where}
  %  \begin{center}

    $$
  \begin{array}{l}
    S~x =
    \begin{cases}
      1, & \text{if } x\geq 0\\
      0, & \text{otherwise}
    \end{cases}
  \end{array}
$$
  %\end{center}
\end{alertblock}}

%(This is one of infinitely many solutions)
    
\end{frame}

%\begin{frame}
%\frametitle{A declarative view on neural networks, by example}
% \end{frame}


\begin{frame}
  \frametitle{Activation functions}

    \begin{center}
    
          \includegraphics[scale=.07]{Images/af.png}
    \end{center}


  \end{frame}

\begin{frame}
  \frametitle{Shape of Data versus shape of Neural Nets}
 \begin{columns}
   \column{.5\textwidth}
  \uncover<1->{ \begin{block}{}
     Different Data sets...
       \includegraphics[scale=.40]{Images/mnist_example_picture.png}
     \end{block}}
   
 
   \column{.5\textwidth}
  \uncover<2->{ \begin{block}{may call for}
      different shapes of networks
           \end{block}
       \includegraphics[scale=.40]{Images/neural_network_aplas.png}
}
\end{columns}
   
 \end{frame}

\begin{frame}
\frametitle{Neural networks}

\begin{alertblock}{... are ideal for  ``perception'' tasks:}
    \setbeamercovered{transparent}
\begin{itemize}[<+->]
  \item approximate functions when exact solution is hard to get
    \item tolerant to noisy and incomplete data
    \end{itemize}
\end{alertblock}  
\pause

\begin{block}{BUT}
  \begin{itemize}
   % \item continuous decision space means solutions can only be approximate 
	\item solutions not easily conceptualised (\alert{lack of explainability})
	%\item prone to error
	\item prone to a new range of safety and security problems: \pause
          \begin{itemize} \item[] adversarial attacks
        \item[]  data poisoning
        \item[] catastrophic forgetting
          \end{itemize}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
  \frametitle{Adversarial Attacks}

 % \begin{block}{}
 %   Given a trained neural network and a correctly classified image of ``0'' on the left,
 %                 we can create a perturbation $\eta$ (middle) to the original image so that the same neural network predicts a ``5" with 92\% confidence for the modified image (right).
 %   \end{block}
  
  	\includegraphics[width=.3\textwidth]{Images/true.png}
	\uncover<2->{\includegraphics[width=.3\textwidth]{Images/eta.png}}
	\uncover<3->{\includegraphics[width=.3\textwidth]{Images/adv.png}}

\begin{itemize}
\item<4->[] the perturbations are imperceptible to human eye \pause
\item<5->[] attacks transfer from one neural network to another \pause
\item<6->[] affect any domain where neural networks are applied                 
\end{itemize}
                
\end{frame}


    
%\subsection{How is it different?}

\begin{frame}
  \frametitle{A few words on the context}

  \begin{itemize}
  \item[1943] Perceptron by McCullogh and Pitts \pause
  \item[90s --] Rise of machine learning applications \pause
  \item[2013]
   {\scriptsize
 \begin{thebibliography}{99}
 \beamertemplatearticlebibitems
  \bibitem{2}{C.~Szegedy, W.~Zaremba, I.~Sutskever, J.~Bruna, D.~Erhan,
I.~Goodfellow, and R.~Fergus.  Intriguing properties of neural networks. 2013. (5000+ citations)}
\end{thebibliography}}

\small{\emph{`` The existence of the adversarial negatives appears to be in contradiction with the network’s ability to achieve high generalization performance.   Indeed,  if the network can generalize well,  how can it be confused by these adversarial negatives,  which are indistinguishable from the regular examples?  ``}}


  \end{itemize}
    \end{frame}

\begin{frame}
  \frametitle{A few words on the context}

  \begin{itemize}
  \item[1943] Perceptron by McCullogh and Pitts
  \item[90-2000] Rise of machine learning applications
  \item[2013]
   {\scriptsize
 \begin{thebibliography}{99}
 \beamertemplatearticlebibitems
  \bibitem{2}{C.~Szegedy, W.~Zaremba, I.~Sutskever, J.~Bruna, D.~Erhan,
I.~Goodfellow, and R.~Fergus.  Intriguing properties of neural networks. 2013. (5000+ citations)}
\end{thebibliography}}

\item[2013-..] Thousands of papers on adversarial training

  (in the attack-defence style)
    {\scriptsize
 \begin{thebibliography}{99}
   \beamertemplatearticlebibitems
   \bibitem{3}{A.~C.~Serban, E.~Poll, J.~Visser.
Adversarial Examples - A Complete Characterisation of the Phenomenon. 2019.}
\end{thebibliography}}

\pause

\item[2017] First Neural network verification attempts
    {\scriptsize
 \begin{thebibliography}{99}
   \beamertemplatearticlebibitems
    \bibitem{4}{G.~Katz, C.W.~Barrett, D.L.~Dill, K.~Julian, M.J.~Kochenderfer:
      Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks. CAV (1) 2017: 97-117.}
    \bibitem{5}{ X. Huang, M. Kwiatkowska, S. Wang, M. Wu. Safety Verification of Deep Neural Networks. CAV (1) 2017: 3-29.}
\end{thebibliography}}


\item[2017-..] Hundreds of papers on neural network verification

  \end{itemize}
\end{frame}

\begin{frame}
\frametitle{}


\begin{center}

\includegraphics[width=6cm]{Images/question.jpeg}

\end{center}
\end{frame}


\section{Challenges of Neural Network Verification}


    



\begin{frame}
  \frametitle{Programs are functions...}

    \begin{alertblock}{ $Program: \mathcal{A} \rightarrow \mathcal{B}$}
    %$$NeuralNet: \Real^n \rightarrow \Real^m$$
   % where $n$ is the size (or \emph{dimension}) of inputs, and $m$ -- the number of \emph{classes}. 
  \end{alertblock}
  
and so are neural networks:
  
  \begin{alertblock}{}
    $$NeuralNet: \Real^n \rightarrow \Real^m$$
   % where $n$ is the size (or \emph{dimension}) of inputs, and $m$ -- the number of \emph{classes}. 
  \end{alertblock}

%  \pause

 % \begin{center}
 %   \alert{BUT?..}
 %   \end{center}
  
\end{frame}

\begin{frame}
  \frametitle{Neural Network Verification}
  
  \begin{block}{... could be like any other verification task}
    \pause
if not for the following four problems:
\end{block}


  
  \begin{center}
  \setbeamercovered{transparent}
\begin{itemize}[<+->]
    \item[I] Semantics of function components is mostly opaque
    \item[II] Number of verification parameters is huge
    \item[III] Undecidable verification for non-linear functions
    \item[IV] Finding verifiable neural networks may be difficult
    \end{itemize}
\end{center}
    
\end{frame}

\begin{frame}
  \frametitle{I. The problem of opaque semantics}

  \begin{alertblock}{ $Program: \mathcal{A} \rightarrow \mathcal{B}$}
    $$NeuralNet: \Real^n \rightarrow \Real^m$$
   % where $n$ is the size (or \emph{dimension}) of inputs, and $m$ -- the number of \emph{classes}. 
  \end{alertblock}

  
  \begin{block}{But normally, programs}
    \begin{itemize}
    \item[] have semantically meaningful parts
      \item[] which allows us to verify components that matter
      \end{itemize}
\end{block}
\end{frame}

\begin{frame}
  \frametitle{I. The problem of opaque semantics}

  \begin{alertblock}{   $Program: \mathcal{A} \rightarrow \mathcal{B}$}
     $$NeuralNet: \Real^n \rightarrow \Real^m$$
   % where $n$ is the size (or \emph{dimension}) of inputs, and $m$ -- the number of \emph{classes}. 
  \end{alertblock}

  
  \begin{block}{For neural nets:}
    \begin{itemize}
    \item[] \xout{have semantically meaningful parts}
      \item[] \xout{which allows us to verify components that matter}
      \end{itemize}
\end{block}



\end{frame}


\begin{frame}
  \frametitle{I. The problem of opaque semantics}

  \begin{alertblock}{   $Program: \mathcal{A} \rightarrow \mathcal{B}$}
     $$NeuralNet: \Real^n \rightarrow \Real^m$$
   % where $n$ is the size (or \emph{dimension}) of inputs, and $m$ -- the number of \emph{classes}. 
  \end{alertblock}

  
  \begin{block}{For neural nets:}
    \begin{itemize}
    \item[] input and output are the only semantically meaningful parts
      (and even that is somewhat blurry)
           \end{itemize}
\end{block}



\end{frame}


    \begin{frame}
      \frametitle{The ``$\epsilon$-ball verification''}
\begin{center}
  \begin{tikzpicture}[scale=.8]
  % draw the sphere
  \shade[ball color = gray!40, opacity = 0.4] circle (2cm);
  \draw (0,0) circle (2cm);
  \draw (-2,0) arc (180:360:2 and 1);
  \draw[dashed] (2,0) arc (0:180:2 and 1);
  \fill[fill=black] (0,0) circle (1pt);
  \draw[dashed,<->] (0,0 ) -- node[above]{$\epsilon$} (2,0);
  % line for original image
  \draw[] (0,0 ) -- node{} (0,-2.5);
  \node (1orig) at (0,-3) {\includegraphics{Images/7_orig.png}};
 % \node[] at (0,-3.8) {NN: ``seven''};
  % line for the perturbed image 1
  \draw[] (-0.6,1.2) -- node{} (-0.9,2.7);
  \node () at (-1,2.8) {\includegraphics{Images/7_v1}};
  % line for the perturbed image 2
  \draw[] (0.6,1.4) -- node{} (1.2,2.7);
  \node () at (1.2,2.8) {\includegraphics{Images/7_v2}};
  % line for the perturbed image 3
  \draw[] (0.6,-1.2) -- node{} (2.5,-2.7);
  \node () at (2.5,-2.8) {\includegraphics{Images/7_v3}};
\end{tikzpicture}
\end{center}

\pause
  \begin{alertblock}{An $\epsilon$-ball     $\mathbb{B}(\hat{x}, \epsilon) = \{ {x \in \mathbb{R}^n: ||\hat{x}-x|| \leq \epsilon} \}$ }


    Classify all  points in $\mathbb{B}(\hat{x}, \epsilon)$ in the \alert{``same class''} as $\hat{x}$.
  \end{alertblock}

 % \begin{block}{}
%\end{block}
    \end{frame}

    

\begin{frame}
  \frametitle{For example,}

    \begin{block}{Take}
$$
\begin{array}{l}
  \neuron : (x_1:\Real) \to (x_2: \Real) \to (y: \Real\;\{y=0 \lor y=1\})\\
  \neuron \; x_1 \; x_2 = S \; (-0.9 + 0.5 x_1 + 0.5 x_2)
\end{array}
$$
\end{block}

   \begin{alertblock}{}
    \begin{center}

     $\xymatrix@R=0.5pc@C=0pc{
      &*\txt{$x_1$}\ar[rrrr]&&&&*\txt{$w_{x_1}$}\ar[rrrrd]&&&&&&&&\\
      &*\txt{$x_2$}\ar[rrrr]&&&&*\txt{$w_{x_2}$}\ar[rrrr]&&&&*+++[o][F]{+b}\ar[rrr]&&&*\txt{ $y$ }&\\
    } $
  \end{center}
  \end{alertblock}
    
\end{frame}

\begin{frame}
  \frametitle{For example,}

    \begin{block}{Take}
$$
\begin{array}{l}
  \neuron : (x_1:\Real) \to (x_2: \Real) \to (y: \Real\;\{y=0 \lor y=1\})\\
  \neuron \; x_1 \; x_2 = S \; (-0.9 + 0.5 x_1 + 0.5 x_2)
\end{array}
$$
\end{block}

   \begin{alertblock}{Define}
    \begin{center}
\begin{align*}
  \truthy \; x & = |1 - x| \leq \epsilon \\
  \falsey \; x & = |0 - x| \leq \epsilon
\end{align*}
  \end{center}
  \end{alertblock}
    
\end{frame}

\begin{frame}
  \frametitle{For example,}

    \begin{block}{Take}
$$
\begin{array}{l}
  \neuron : (x_1:\Real) \to (x_2: \Real) \to (y: \Real\;\{y=0 \lor y=1\})\\
  \neuron \; x_1 \; x_2 = S \; (-0.9 + 0.5 x_1 + 0.5 x_2)
\end{array}
$$
\end{block}

   \begin{alertblock}{Verify}
     \begin{center}
       \small{
$$
\setlength{\arraycolsep}{2pt}
\begin{array}{rcl}
  \neurontest & : & (x_1: \Real\;\{\truthy\;x_1\})
                \to (x_2: \Real\;\{\truthy\;x_2\})
                \to (y: \Real\;\{y=1\})\\
  \neurontest & = & \neuron
\end{array}
$$}
  \end{center}
  \end{alertblock}

  \pause

      {\scriptsize
 \begin{thebibliography}{99}
   \beamertemplatearticlebibitems
    \bibitem{6}{Wen Kokke, E.K., Daniel Kienitz, Robert Atkey and
David Aspinall. 2020. Neural Networks, Secure by Construction: An Exploration
of Refinement Types. APLAS'20.}
\end{thebibliography}}
 
\end{frame}


\begin{frame}[fragile]
  \frametitle{Refinement type library for Neural Net Verification}

   \begin{columns}
   \column{.20\textwidth}
  \uncover<1->{ \begin{alertblock}{}
     data \tikzmark{n1}
       \includegraphics[scale=.20]{Images/data.jpeg}
     \end{alertblock}}
%%%%%%%%%%%%%%
     
     \column{.20\textwidth}

  \uncover<2->{\begin{alertblock}{}
      \tikzmark{n2}   Python (Keras)     \tikzmark{n40}       \includegraphics[scale=.20]{Images/python.jpeg}
       \tikzmark{n3}

  \end{alertblock}}

\vspace{1cm}

     \uncover<2->{\begin{block}{}
       a neural network    \tikzmark{n5}
           \includegraphics[scale=.20]{Images/NN.png}
       \end{block}}
%%%%%%%%
     \column{.20\textwidth}

     \uncover<3->{\begin{alertblock}{}
         \tikzmark{n4}  $F^*$ \tikzmark{n10}
         \includegraphics[scale=.20]{Images/f*.png}
           \tikzmark{n41}  
    \end{alertblock}}

   \vspace{1cm}
     \uncover<3->{\begin{block}{}
         NN as function;  \tikzmark{n6}\\
         verification conditions as types
           %\includegraphics[scale=.25]{Images/NN.png}
       \end{block}}
%%%%%%%%
  
     
       \column{.20\textwidth}
    
         \uncover<4->{\begin{alertblock}{}
             \tikzmark{n42}   Z3
             \includegraphics[scale=.20]{Images/z3.jpeg}
               \tikzmark{n43} 
                  \end{alertblock}}

                 \vspace{0.5cm}
                 \uncover<4->{\begin{block}{}
                     \includegraphics[scale=.20]{Images/tick.jpeg}   \tikzmark{n44} 
       \end{block}}
   
   \end{columns}
     
%   \begin{tikzpicture}[remember picture,overlay]

%   \path[draw=blue, thick,->]<2-> (n1) -- (n2);
%   \path[draw=blue, thick,->]<2-> (n40) -- (n5);
%     \path[draw=blue, thick,->]<3-> (n40) -- (n4);
%     \path[draw=blue, thick,->]<3-> (n41) -- (n6);
%          \path[draw=blue, thick,->]<4-> (n10) -- (n42);
%   \path[draw=blue, thick,->]<4-> (n43) -- (n44);

%\end{tikzpicture}
 \end{frame}

 
\begin{frame}[fragile]
  \frametitle{Except you only see:}

   \begin{columns}
   \column{.20\textwidth}
  \uncover<1->{ \begin{alertblock}{}
     data \tikzmark{n1}
       \includegraphics[scale=.20]{Images/data.jpeg}
     \end{alertblock}}
%%%%%%%%%%%%%%
     
     \column{.20\textwidth}



%%%%%%%%
     \column{.20\textwidth}

     \uncover<1->{\begin{alertblock}{}
         \tikzmark{n4}  $F^*$ \tikzmark{n10}
         \includegraphics[scale=.20]{Images/f*.png}
           \tikzmark{n41}  
    \end{alertblock}}

   \vspace{1cm}
     \uncover<1->{\begin{block}{}
         NN as function;  \tikzmark{n6}\\
         verification conditions as types
           %\includegraphics[scale=.25]{Images/NN.png}
       \end{block}}
%%%%%%%%
  
     
       \column{.20\textwidth}
    
    
   \end{columns}
     
%   \begin{tikzpicture}[remember picture,overlay]

%   \path[draw=blue, thick,->]<2-> (n1) -- (n2);
%   \path[draw=blue, thick,->]<2-> (n40) -- (n5);
%     \path[draw=blue, thick,->]<3-> (n40) -- (n4);
%     \path[draw=blue, thick,->]<3-> (n41) -- (n6);
%          \path[draw=blue, thick,->]<4-> (n10) -- (n42);
%   \path[draw=blue, thick,->]<4-> (n43) -- (n44);

%\end{tikzpicture}
\end{frame}


\begin{frame}[fragile]
  \frametitle{Refinement type library for Neural Net Verification}

  \begin{itemize}
  \item[1] Let Python process the data and find a suitable network
    \pause
    \item[2] Export Python neural net to F* automatically:
    \end{itemize}

    \begin{block}{}
      \begin{lstlisting}
val model : network (*with*) 2 (*inputs*) 1 (*output*) 1 (*layer*)
let model = NLast // <- makes single-layer network
  { weights    = [[0.5R]; [0.5R]]
  ; biases     = [~.0.9R]
  ; activation = Threshold }
\end{lstlisting}
\pause
\begin{alertblock}{NB}
  Uniform syntax for all networks we obtain from Python code!
\end{alertblock}      
      \end{block}

  \end{frame}

  \begin{frame}[fragile]
  \frametitle{Refinement type library for Neural Net Verification}

  \begin{itemize}
  \item[1] Let Python process the data and find a suitable network
    
    \item[2] Export Python neural net to F* automatically:
    \end{itemize}

    \begin{block}{}
      \begin{lstlisting}
val model : network (*with*) (*@ \hl{2} @*) (*inputs*) (*@ \hl{1} @*) (*output*) (*@ \hl{1} @*) (*layer*)
let model = (*@ \hl{NLast} @*) // <- makes single-layer network
  { (*@ \hl{weights} @*)    = [[0.5R]; [0.5R]]
  ; (*@ \hl{biases} @*)     = [~.0.9R]
  ; (*@ \hl{activation} @*) = Threshold }
\end{lstlisting}
%\pause
\begin{alertblock}{NB}
  Uniform syntax for all networks we obtain from Python code!
\end{alertblock}      
      \end{block}

  \end{frame}

  
\begin{frame}[fragile]
  \frametitle{Refinement type library for Neural Net Verification}

  \begin{itemize}
  \item[1] Let Python process the data and find a suitable network
   
    \item[2] Export Python neural net to F* automatically:

      \item[3] Define your verification conditions:
          \end{itemize}
    \begin{alertblock}{}
      \begin{lstlisting}
let eps      = 0.1R
let truthy x = 1.0R - eps <=. x && x <=. 1.0R + eps
let falsey x = 0.0R - eps <=. x && x <=. 0.0R + eps

val verify : (x1 : real{truthy x1}) -> (x2 : real{truthy x2})
        -> (y  : vector real 1 {y == [1.0R]})
let verify x1 x2 = run model [x1; x2]
\end{lstlisting}
\end{alertblock}
%\pause
% \begin{itemize}
%      \item[4] Type check and relax!
%          \end{itemize}
%
        \end{frame}

          
\begin{frame}[fragile]
  \frametitle{Refinement type library for Neural Net Verification}

  \begin{itemize}
  \item[1] Let Python process the data and find a suitable network
   
    \item[2] Export Python neural net to F* automatically:

      \item[3] Define your verification conditions:
          \end{itemize}
    \begin{alertblock}{}
      \begin{lstlisting}
let eps      = 0.1R
let truthy x = 1.0R - eps <=. x && x <=. 1.0R + eps
let falsey x = 0.0R - eps <=. x && x <=. 0.0R + eps

val verify : (x1 : real  {(*@\hl{truthy x1}@*)} )
     -> (x2 : real {(*@ \hl{truthy x2}@*)} )
        -> (y  : vector real 1 {(*@\hl{y == [1.0R]}@*)} )
let verify x1 x2 = run (*@\hl{model}@*) [x1; x2]
\end{lstlisting}
\end{alertblock}
\pause
Note: it is a universal property. 
% \begin{itemize}
%      \item[4] Type check and relax!
%          \end{itemize}

  \end{frame}

\begin{frame}[fragile]
  \frametitle{Refinement type library for Neural Net Verification}

  \begin{itemize}
  \item[1] Let Python process the data and find a suitable network
   
    \item[2] Export Python neural net to F* automatically:

      \item[3] Define your verification conditions:
          \end{itemize}
    \begin{alertblock}{}
      \begin{lstlisting}
let eps      = 0.1R
let truthy x = 1.0R - eps <=. x && x <=. 1.0R + eps
let falsey x = 0.0R - eps <=. x && x <=. 0.0R + eps

val verify : (x1 : real{truthy x1}) -> (x2 : real{truthy x2})
        -> (y  : vector real 1 {y == [1.0R]})
let verify x1 x2 = run model [x1; x2]
\end{lstlisting}
\end{alertblock}
%\pause
 \begin{itemize}
      \item[4] Type check and relax!
          \end{itemize}

        \end{frame}

  
\begin{frame}
  \frametitle{Neural net robustness as refinement type}

  
      {\scriptsize
 \begin{thebibliography}{99}
   \beamertemplatearticlebibitems
    \bibitem{6}{Wen Kokke, E.K., Daniel Kienitz, Robert Atkey and
David Aspinall. 2020. Neural Networks, Secure by Construction: An Exploration
of Refinement Types. APLAS'20.}
\end{thebibliography}}
 
  \setbeamercovered{transparent}
\begin{itemize}[<+->]
  \item Builds on the real number library in F*;
  \item  Concise Linear Algebra module;
  \item Straightforward definitions of neural nets as composed functions;  
\item with linear or ``non-linear'' activation functions
\item A Python wrapper. 
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{}


\begin{center}

\includegraphics[width=6cm]{Images/question.jpeg}

\end{center}
\end{frame}


\begin{frame}
  \frametitle{Neural Network Verification}
  
 \begin{block}{Recall the 4 problems:}

\end{block}
  
  \begin{center}
    \begin{itemize}
    \item[I] Semantics of function components is opaque
    \item[II] Number of verification parameters is huge
    \item[III] Undecidable verification for non-linear functions
          \item[IV] Finding verifiable neural networks is difficult
    \end{itemize}
\end{center}
    
\end{frame}



\begin{frame}
  \frametitle{Neural Network Verification}
  
 \begin{block}{Recall the 4 problems:}

\end{block}
  
  \begin{center}
    \begin{itemize}
    \item[I] Semantics of function components is  opaque
      \begin{alertblock}{Use refinement types (for [functional] elegance),}
        ... or SMT solvers directly.
      \end{alertblock}

            {\scriptsize
 \begin{thebibliography}{99}
   \beamertemplatearticlebibitems
    \bibitem{6}{Wen Kokke. 2020. Sapphire: a Neural Net Verification Library for Z3 in Python. \url{https://github.com/wenkokke/sapphire}}
\end{thebibliography}}
    
      
    \item[II] Number of verification parameters is huge
    \item[III] Undecidable verification for non-linear functions
          \item[IV] Finding verifiable neural networks 
    \end{itemize}


\end{center}
    
\end{frame}

\begin{frame}[fragile]
  \frametitle{We get}

 \begin{columns}
   \column{.3\textwidth}
  \uncover<1->{ \begin{block}{}
     Semantic Opacity \tikzmark{n1}
      % \includegraphics[scale=.25]{Images/neural_network_aplas.png}
     \end{block}}
%%%%%%%%%%%%%%
     
     \column{.3\textwidth}

  \uncover<2->{\begin{block}{}
    \tikzmark{n2}   $\epsilon$-ball verification  \tikzmark{n3}
  \end{block}}


     \column{.3\textwidth}

     \uncover<3->{\begin{block}{}
      \tikzmark{n4}  overwhelming No of parameters  \tikzmark{n10}
    \end{block}}

   \end{columns}
     
   \begin{tikzpicture}[remember picture,overlay]
        %\path[draw=magenta,thick,->]<3-> ([yshift=3mm]n1) to ++(0,3mm) to [out=0, in=0,distance=2.5in] (t1);
   \path[draw=magenta,ultra thick,->]<2-> (n1) -- (n2);
% \end{tikzpicture}
  %  \begin{tikzpicture}[remember picture,overlay]
        %\path[draw=magenta,thick,->]<3-> ([yshift=3mm]n1) to ++(0,3mm) to [out=0, in=0,distance=2.5in] (t1);
   \path[draw=magenta,ultra thick,->]<3-> (n3) -- (n4);
\end{tikzpicture}
 \end{frame}
 

\begin{frame}
  \frametitle{Neural Network Verification}
  
 \begin{block}{Recall}

\end{block}
  
  \begin{center}
    \begin{itemize}
    \item[I] Semantics of function components is opaque
      \begin{alertblock}{Use refinement types (for [functional] elegance)}
        ... or SMT solvers directly.
        \end{alertblock}
      
    \item[II] \hl{Number of verification parameters is huge}
    \item[III] Undecidable verification for non-linear functions
             \item[IV] Finding verifiable neural networks may be difficult
    \end{itemize}
\end{center}
    
\end{frame}


    \begin{frame}
      \frametitle{The ``$\epsilon$-ball verification''}
\begin{center}
  \begin{tikzpicture}[scale=.8]
  % draw the sphere
  \shade[ball color = gray!40, opacity = 0.4] circle (2cm);
  \draw (0,0) circle (2cm);
  \draw (-2,0) arc (180:360:2 and 1);
  \draw[dashed] (2,0) arc (0:180:2 and 1);
  \fill[fill=black] (0,0) circle (1pt);
  \draw[dashed,<->] (0,0 ) -- node[above]{$\epsilon$} (2,0);
  % line for original image
  \draw[] (0,0 ) -- node{} (0,-2.5);
  \node (1orig) at (0,-3) {\includegraphics{Images/7_orig.png}};
 % \node[] at (0,-3.8) {NN: ``seven''};
  % line for the perturbed image 1
  \draw[] (-0.6,1.2) -- node{} (-0.9,2.7);
  \node () at (-1,2.8) {\includegraphics{Images/7_v1}};
  % line for the perturbed image 2
  \draw[] (0.6,1.4) -- node{} (1.2,2.7);
  \node () at (1.2,2.8) {\includegraphics{Images/7_v2}};
  % line for the perturbed image 3
  \draw[] (0.6,-1.2) -- node{} (2.5,-2.7);
  \node () at (2.5,-2.8) {\includegraphics{Images/7_v3}};
\end{tikzpicture}
\end{center}


%  \begin{alertblock}{An $\epsilon$-ball     $\mathbb{B}(\hat{x}, \epsilon) = \{ {x \in \mathbb{R}^n: ||\hat{x}-x|| \leq \epsilon} \}$ }


 %   Classify all  points in $\mathbb{B}(\hat{x}, \epsilon)$ in the same class as $\hat{x}$.
 % \end{alertblock}

 % \begin{block}{}
%\end{block}
    \end{frame}


    
\begin{frame}
  \frametitle{Scale in Neural network verification }

    
  \begin{block}{MNIST data set}
    \begin{center}
      $28 × 28$ images of the handwritten digits “0” to
    “9”

    $784$ pixels each

    \end{center}
  \end{block}
\pause

 
 \begin{columns}
\column{.4\textwidth}
  \begin{block}{The smallest network}

\begin{itemize}
\item[input] layer of 784 weights
\pause
\item[hidden] layer of (say) 128 ReLU nodes
\pause
\item[output] layer of $10$ softmax neurons
\end{itemize}    
\end{block}


\column{.4\textwidth} 

  \begin{center}
    
          \includegraphics[scale=.25]{Images/neural_network_aplas.png}
    \end{center}

     
 \end{columns}

  
\end{frame}


\begin{frame}[fragile]
  \frametitle{Scaling Neural network verification }
  \begin{center}
    
          \includegraphics[scale=.20]{Images/neural_network_aplas.png}
    \end{center}

 \begin{alertblock}{We want to say:}
\begin{lstlisting}
val sample_in : vector real 784
let sample_in = let v = [7.394R; ~.0.451R; ...; 0.199R]
\end{lstlisting}         
\begin{lstlisting}
val sample_out: vector real 10
let sample_out = let v = [0.998R; 0.000R; ...; 0.000R]
\end{lstlisting}
\end{alertblock}
\pause
\begin{alertblock}{And prove the $\epsilon$-ball property:}
\begin{lstlisting}
   let _ =  forall (x:vector real 784). (|sample_in - x| <. 0.01R)
   ==> (|sample_out - (run network x)| <. 0.1R))
   \end{lstlisting}
\end{alertblock}


\end{frame}


\begin{frame}[fragile]
  \frametitle{Scaling Neural network verification }
  \begin{center}
    
          \includegraphics[scale=.20]{Images/neural_network_aplas.png}
    \end{center}

 \begin{alertblock}{We want to say:}
\begin{lstlisting}
val sample_in : vector real 784
let sample_in = let v = [7.394R; ~.0.451R; ...; 0.199R]
\end{lstlisting}         
\begin{lstlisting}
val sample_out: vector real 10
let sample_out = let v = [0.998R; 0.000R; ...; 0.000R]
\end{lstlisting}
\end{alertblock}

 \begin{alertblock}{And prove the $\epsilon$-ball property:}
\begin{lstlisting}
let _ = (*@  \hl{$\forall$ (x:vector $\Real$ 784).} @*)(|sample_in - x| <. 0.01R)
==> (|sample_out - (run network x)| <. 0.1R))
   \end{lstlisting}
\end{alertblock}
  
\end{frame}

\begin{frame}[fragile]
  \frametitle{Scaling Neural network verification }
  \begin{center}
    
          \includegraphics[scale=.20]{Images/neural_network_aplas.png}
    \end{center}

 \begin{alertblock}{We want to say:}
\begin{lstlisting}
val sample_in : vector real 784
let sample_in = let v = [7.394R; ~.0.451R; ...; 0.199R]
\end{lstlisting}         
\begin{lstlisting}
val sample_out: vector real 10
let sample_out = let v = [0.998R; 0.000R; ...; 0.000R]
\end{lstlisting}
\end{alertblock}

\begin{alertblock}{And prove the $\epsilon$-ball property:}
  \begin{lstlisting}
let _ =  forall (x:vector real 784). (*@ \hl{(|sample\_in - x| $<$ 0.01R} @*) 
 ==> (|sample_out - (run network x)| <. 0.1R))
\end{lstlisting}
\end{alertblock}
  
\end{frame}


\begin{frame}[fragile]
  \frametitle{Scaling Neural network verification }
  \begin{center}
    
          \includegraphics[scale=.20]{Images/neural_network_aplas.png}
    \end{center}

 \begin{alertblock}{We want to say:}
\begin{lstlisting}
val sample_in : vector real 784
let sample_in = let v = [7.394R; ~.0.451R; ...; 0.199R]
\end{lstlisting}         
\begin{lstlisting}
val sample_out: vector real 10
let sample_out = let v = [0.998R; 0.000R; ...; 0.000R]
\end{lstlisting}
\end{alertblock}

\begin{alertblock}{And prove  the $\epsilon$-ball property:}
  \begin{lstlisting}
let _ =  forall (x:vector real 784). (|sample_in - x| <. 0.01R)
==> (|sample_out - (*@ \hl{(run network x)} @*) | <. 1.0R))
\end{lstlisting}
\end{alertblock}
  
\end{frame}
    
\begin{frame}
  \frametitle{Scaling Neural network verification }
  \begin{center}
    
          \includegraphics[scale=.40]{Images/neural_network_aplas.png}
    \end{center}


  \begin{block}{The smallest reasonable neural network}

\begin{itemize}
\item[input] layer of 784 weights
\item[hidden] layer of (say) 128 ReLU nodes
\item[output] layer of $10$ softmax neurons
\pause
  \item[total] $784 \times 128 + 128 +
128 \times 10 + 10 = 101770$ parameters 
%\pause 
%\item[] because layers are fully connected,

%  each input parameter occurs at least
%128 × 10 = 1280 times in the SMT query
\end{itemize}    
\end{block}
\end{frame}


\begin{frame}
  \frametitle{Neural Network Verification}
  
 \begin{block}{Recall}

\end{block}
  
  \begin{center}
    \begin{itemize}
    \item[I] Semantics of function components is  opaque
    \item[II] Number of verification parameters is huge
    \item[III] Undecidable verification for non-linear functions
             \item[IV] Finding verifiable neural networks may be difficult
    \end{itemize}
\end{center}
    
\end{frame}


\begin{frame}
  \frametitle{Neural Network Verification}
  
 \begin{block}{Recall}

\end{block}
  
  \begin{center}
    \begin{itemize}
    \item[I] Semantics of function components is mostly opaque
    \item[II] Number of verification parameters is huge
      \begin{alertblock}{Reduce the number of parameters (to scale)}
        \begin{itemize}
        \item[either] reduce network size and re-train
        \item[or] reduce the network to a provably equivalent
          \item[or] use over-approximation (in the style of abstract interpretation)
          \end{itemize}
      \end{alertblock}
      
    \item[III] Undecidable verification for non-linear functions
             \item[IV] Finding verifiable neural networks may be difficult
    \end{itemize}
\end{center}
    
\end{frame}


\begin{frame}
  \frametitle{Neural Network Verification}
  
 \begin{block}{Recall}

\end{block}
  
  \begin{center}
    \begin{itemize}
    \item[I] Semantics of function components is mostly opaque
    \item[II] Number of verification parameters is huge
      \begin{alertblock}{Reduce the number of parameters (to scale)}
        \begin{itemize}
        \item[either] reduce network size and re-train
                {\scriptsize
 \begin{thebibliography}{99}
   \beamertemplatearticlebibitems
    \bibitem{6}{W.Kokke, E.K., D.Kienitz, R.Atkey and
D.Aspinall. 2020. Neural Networks, Secure by Construction: An Exploration
of Refinement Types. APLAS'20.}
\end{thebibliography}}
         \item[or] reduce the network to a provably equivalent

         \item[or] use over-approximation (in the style of abstract interpretation)
          \end{itemize}
      \end{alertblock}
      
    \item[III] Undecidable verification for non-linear functions
             \item[IV] Finding verifiable neural networks may be difficult
    \end{itemize}
\end{center}
    
\end{frame}


\begin{frame}
  \frametitle{Neural Network Verification}
  
 \begin{block}{Recall}

\end{block}
  
  \begin{center}
    \begin{itemize}
    \item[I] Semantics of function components is mostly opaque
    \item[II] Number of verification parameters is huge
      \begin{alertblock}{Reduce the number of parameters (to scale)}
        \begin{itemize}
        \item[either] reduce network size and re-train
          
         \item[or] reduce the network to a provably equivalent
      {\scriptsize
 \begin{thebibliography}{99}
   \beamertemplatearticlebibitems
    \bibitem{9}{S.Gokulanathan, A.Feldsher, A.Malca, C.W. Barrett, G. Katz:
Simplifying Neural Networks Using Formal Verification. NFM 2020: 85-93}
\end{thebibliography}}
         \item[or] use over-approximation (in the style of abstract interpretation)
          \end{itemize}
      \end{alertblock}
      
    \item[III] Undecidable verification for non-linear functions
             \item[IV] Finding verifiable neural networks may be difficult
    \end{itemize}
  \end{center}
    
\end{frame}


\begin{frame}
  \frametitle{Neural Network Verification}
  
 \begin{block}{Recall}

\end{block}
  
  \begin{center}
    \begin{itemize}
    \item[I] Semantics of function components is mostly opaque
    \item[II] Number of verification parameters is huge
      \begin{alertblock}{Reduce the number of parameters (to scale)}
        \begin{itemize}
        \item[either] reduce network size and re-train
          
         \item[or] reduce the network to a provably equivalent
     
         \item[or] use over-approximation (in the style of abstract interpretation)
            {\scriptsize
 \begin{thebibliography}{99}
   \beamertemplatearticlebibitems
    \bibitem{9}{	G.Singh, T.Gehr, M.Püschel, M.Vechev:
An abstract domain for certifying neural networks. Proc. ACM Program. Lang. 3(POPL): 41:1-41:30 (2019)
}
\end{thebibliography}}
          \end{itemize}
      \end{alertblock}
      
    \item[III] Undecidable verification for non-linear functions
       \item[IV] Finding verifiable neural networks may be difficult
      \end{itemize}
\end{center}
    
\end{frame}


\begin{frame}[fragile]
  \frametitle{So far...}

 \begin{columns}
   \column{.3\textwidth}
  \uncover<1->{ \begin{block}{}
     Semantic Opacity \tikzmark{n1}
       \includegraphics[scale=.25]{Images/neural_network_aplas.png}
     \end{block}}
%%%%%%%%%%%%%%
     
     \column{.3\textwidth}

  \uncover<2->{\begin{block}{}
    \tikzmark{n2}   $\epsilon$-ball verification  \tikzmark{n3}
  \end{block}}

\vspace{1cm}

     \uncover<4->{\begin{alertblock}{}
       \tikzmark{n5} can verify a  \tikzmark{n7}  different object! 
       \end{alertblock}}
%%%%%%%%
     \column{.3\textwidth}

     \uncover<3->{\begin{block}{}
      \tikzmark{n4}  overwhelming No of parameters  \tikzmark{n10}
    \end{block}}
  
%\vspace{0.5cm}
    
         \uncover<5->{\begin{alertblock}{}
             \tikzmark{n6}   verify a  \tikzmark{n8} smaller network
                    \includegraphics[scale=.25]{Images/neural_network_aplas2.png}
     \end{alertblock}}

   \end{columns}
     
   \begin{tikzpicture}[remember picture,overlay]
        %\path[draw=magenta,thick,->]<3-> ([yshift=3mm]n1) to ++(0,3mm) to [out=0, in=0,distance=2.5in] (t1);
   \path[draw=magenta,ultra thick,->]<2-> (n1) -- (n2);
% \end{tikzpicture}
  %  \begin{tikzpicture}[remember picture,overlay]
        %\path[draw=magenta,thick,->]<3-> ([yshift=3mm]n1) to ++(0,3mm) to [out=0, in=0,distance=2.5in] (t1);
   \path[draw=magenta,ultra thick,->]<3-> ([xshift=3mm]n3) -- (n4);
   \path[draw=magenta,ultra thick,->]<4-> (n1) -- (n5);
 %  \path[draw=magenta,ultra thick,->]<5-> (n10) -- (n8);
         \path[draw=magenta,ultra thick,->]<5-> (n7) -- (n6);
\end{tikzpicture}

        \end{frame}



\begin{frame}
\frametitle{}


\begin{center}

\includegraphics[width=6cm]{Images/question.jpeg}


\includegraphics[width=6cm]{Images/pandora.jpeg}

\end{center}
\end{frame}


        
\begin{frame}
  \frametitle{Neural Network Verification}
  
 \begin{block}{Recall}

\end{block}
  
  \begin{center}
    \begin{itemize}
    \item[I] Semantics of function components is mostly opaque
    \item[II] Number of verification parameters is huge
    \item[III] \hl{Undecidable verification for non-linear functions}
         \item[IV] Finding verifiable neural networks may be difficult

    \end{itemize}
\end{center}
    
\end{frame}




\begin{frame}
  \frametitle{Activation functions}

    \begin{center}
    
          \includegraphics[scale=.07]{Images/af.png}
    \end{center}


  \end{frame}

  \begin{frame}
  \frametitle{On the solvers side...}

   \begin{columns}
   \column{.4\textwidth}
  \begin{block}{The SMT solver Z3:}
    \begin{itemize}
    \item  uses Dual Simplex to solve \hl{linear real arithmetic};
\item  and a \hl{fragment of non-linear real arithmetic} -- multiplications
\item  uses conflict resolution procedure
  \item based on
cylindrical algebraic decomposition
      \end{itemize}
    \end{block}
 
    \column{.4\textwidth}
\uncover<3->{
    \begin{block}{We need:}
    \begin{itemize}
    \item  exponents
    \item logarithms
      \item trigonometric functions
      \end{itemize}
    \end{block}}
\end{columns}
         \uncover<2-> {{\scriptsize
 \begin{thebibliography}{99}
   \beamertemplatearticlebibitems
    \bibitem{9}{Jovanović, D., de Moura, L.: Solving non-linear arithmetic. ACM
Communications in Computer Algebra 46(3/4),
104 (Jan 2013).
}
\end{thebibliography}}}
\end{frame}

 \begin{frame}
  \frametitle{On the solvers side...}

   \begin{columns}
   \column{.4\textwidth}
  \begin{block}{The solver  MetiTarski:}
Supports:
    \begin{itemize}
    \item  exponents
    \item logarithms
      \item trigonometric functions
      \end{itemize}
      \hl{for 4-5 variables}
\end{block}
      
    \column{.4\textwidth}
\uncover<2->{
    \begin{block}{We need:}
\hl{hundreds of variables}
    \end{block}}
\end{columns}
         \uncover<1-> {{\scriptsize
 \begin{thebibliography}{99}
   \beamertemplatearticlebibitems
    \bibitem{9}{Akbarpour, B., Paulson, L.C.: MetiTarski: An automatic theorem prover for real-
valued special functions. Journal of Automated Reasoning 44(3), 175–205 (Aug
2009).
}
\end{thebibliography}}}

\end{frame}

\begin{frame}
  \frametitle{Solutions?!}
  \begin{alertblock}{Linearise effectively!}
    \end{alertblock}


	\includegraphics[width=0.33333\textwidth]{Images/sigmoid1.png}%
	\includegraphics[width=0.33333\textwidth]{Images/sigmoid5.png}%
	\includegraphics[width=0.33333\textwidth]{Images/sigmoid25.png}


        
                {\scriptsize
 \begin{thebibliography}{99}
   \beamertemplatearticlebibitems
    \bibitem{6}{Wen Kokke, E.K., Daniel Kienitz, Robert Atkey and
David Aspinall.  Neural Networks, Secure by Construction: An Exploration
of Refinement Types. APLAS'20.}
\end{thebibliography}}

\end{frame}

\begin{frame}
  \frametitle{Solutions?!}
  \begin{alertblock}{Linearise effectively!}
    \end{alertblock}


        	\includegraphics[width=0.33333\textwidth]{Images/tanh_extrapolate.png}%
	\includegraphics[width=0.33333\textwidth]{Images/tanh_hard.png}%
	\includegraphics[width=0.33333\textwidth]{Images/tanh_margin.png}

        
                {\scriptsize
 \begin{thebibliography}{99}
   \beamertemplatearticlebibitems
    \bibitem{6}{Wen Kokke, E.K., Daniel Kienitz, Robert Atkey and
David Aspinall.  Neural Networks, Secure by Construction: An Exploration
of Refinement Types. APLAS'20.}
\end{thebibliography}}

\end{frame}  

\begin{frame}
  \frametitle{Neural Network Verification}
  
 \begin{block}{Recall}

\end{block}
  
  \begin{center}
    \begin{itemize}
    \item[I] Semantics of function components is mostly opaque
            \begin{alertblock}{Use refinement types for functional elegance}
            \end{alertblock}
            \pause
    \item[II] Number of verification parameters is huge
      \begin{alertblock}{Reduce the number of parameters for scale}
      \end{alertblock}
      \pause
      \item[III] Undecidable verification for non-linear functions
           \begin{alertblock}{Linearise effectively for automation}
           \end{alertblock}
           \pause
         \item[IV] Finding verifiable neural networks may be difficult
    \end{itemize}
\end{center}
    
\end{frame}

\begin{frame}[fragile]
  \frametitle{``Opaque Verification''}

 \begin{columns}
   \column{.3\textwidth}

     \uncover<1->{\begin{block}{}
      \tikzmark{n4}  overwhelming No of parameters  \tikzmark{n10}
    \end{block}}
  

     \uncover<2->{ \begin{block}{}
       Non-linearity\tikzmark{n11}
         	\includegraphics[scale=.2]{Images/sigmoid1.png}
    %   \includegraphics[scale=.25]{Images/neural_network_aplas.png}
     \end{block}}

   
%%%%%%%%%%%%%%
     
     \column{.3\textwidth}

  %\uncover<1->{ \begin{block}{}
  %   Semantic  \tikzmark{n12} Opacity \tikzmark{n1}
    %   \includegraphics[scale=.25]{Images/neural_network_aplas.png}
  %   \end{block}}

\vspace{1cm}

     \uncover<1->{\begin{alertblock}{}
       \tikzmark{n5} can verify  \tikzmark{n23} a  \tikzmark{n7}  different object!  \tikzmark{n14} 
       \end{alertblock}}
%%%%%%%%
     \column{.3\textwidth}

   
%\vspace{0.5cm}
    
         \uncover<1->{\begin{alertblock}{}
             \tikzmark{n6}   verify a  \tikzmark{n8} smaller network
                   % \includegraphics[scale=.25]{Images/neural_network_aplas2.png}
           \end{alertblock}}
         
         \uncover<4->{\begin{alertblock}{}
             \tikzmark{n13}   verify a  \tikzmark{n8} linearised network
                    \includegraphics[scale=.2]{Images/sigmoid25.png}
     \end{alertblock}}

   \end{columns}
     
   \begin{tikzpicture}[remember picture,overlay]
        %\path[draw=magenta,thick,->]<3-> ([yshift=3mm]n1) to ++(0,3mm) to [out=0, in=0,distance=2.5in] (t1);
  % \path[draw=blue, thick,->]<1-> (n1) -- (n2);
% \end{tikzpicture}
  %  \begin{tikzpicture}[remember picture,overlay]
        %\path[draw=magenta,thick,->]<3-> ([yshift=3mm]n1) to ++(0,3mm) to [out=0, in=0,distance=2.5in] (t1);
   %\path[draw=blue, thick,->]<1-> ([xshift=3mm]n3) -- (n4);
   %\path[draw=blue, thick,->]<1-3> (n12) -- (n23);
   \path[draw=blue, thick,->]<1-> (n10) -- (n5);
   \path[draw=blue, thick,->]<1-> (n7) -- (n6);
  % \path[draw=magenta,ultra thick,->]<4-> (n12) -- (n23);
   %\path[draw=magenta, ultra thick,->]<4-> (n1) -- (n5);
   %\path[draw=magenta, ultra thick,->]<4-> (n11) -- (n5);
          \path[draw=magenta, ultra thick,->]<3-> (n11) -- (n5);
       \path[draw=magenta, ultra thick,->]<4-> (n14) -- (n13);
   
\end{tikzpicture}

        \end{frame}

        



\begin{frame}
  \frametitle{Neural Network Verification}
  
 \begin{block}{Recall}

\end{block}
  
  \begin{center}
    \begin{itemize}
    \item[I] Semantics of function components is mostly opaque
            \begin{alertblock}{Use refinement types for functional elegance}
            \end{alertblock}
            %\pause
    \item[II] Number of verification parameters is huge
      \begin{alertblock}{Reduce the number of parameters for scale}
      \end{alertblock}
      %\pause
      \item[III] Undecidable verification for non-linear functions
           \begin{alertblock}{Linearise effectively for automation}
           \end{alertblock}
       %    \pause
         \item[IV] Finding verifiable neural networks may be difficult
\pause
                      \begin{alertblock}{(Re-)Train your network correct}
           \end{alertblock}
           
    \end{itemize}
\end{center}
    
\end{frame}


\begin{frame}
  \frametitle{Constraint-driven training}

  \begin{alertblock}{ Train your network correct!}
  
  \begin{itemize}
  \item augment loss functions with logical constraints
                {\scriptsize
 \begin{thebibliography}{99}
   \beamertemplatearticlebibitems
    \bibitem{6}{M.Fischer, M.Balunovic, D.Drachsler-Cohen, T.Gehr, C.Zhang,
and M.Vechev. 2019. DL2: Training and Querying Neural Networks with
Logic. ICML 2019, Vol. 97. PMLR,
1931–1941.}
\end{thebibliography}}
\pause
  \item augment loss functions with abstract interpretation constraints
                {\scriptsize
 \begin{thebibliography}{99}
   \beamertemplatearticlebibitems
      \bibitem{6}{E.Ayers, F.Eiras, M.Hawasly, I.Whiteside:
PaRoT: A Practical Framework for Robust Deep Neural Network Training. NFM 2020: 63-84
}
    \bibitem{6}{M.Balunovic and M.Vechev. 2020. Adversarial Training and Provable
Defenses: Bridging the Gap. ICLR 2020.}
\end{thebibliography}}
    
  \end{itemize}



\end{alertblock}
%\pause
%\begin{block}{A word of caution}
%  \uncover<3>{
%\footnotesize{
%  \begin{thebibliography}{99}
%   \beamertemplatearticlebibitems
%      \bibitem{6}{Marco Casadio. Generative versus logical training
%against adversarial attacks. MSc Thesis at HWU. 2020.}
%\end{thebibliography}}}
%\end{block}

\end{frame}


\begin{frame}[fragile]
  \frametitle{``Opaque Verification''}

 \begin{columns}
   \column{.3\textwidth}

     \uncover<1->{\begin{block}{}
      \tikzmark{n4}  overwhelming No of parameters  \tikzmark{n10}
    \end{block}}
  

     \uncover<1->{ \begin{block}{}
       Non-linearity\tikzmark{n11}
         %	\includegraphics[scale=.2]{Images/sigmoid1.png}
    %   \includegraphics[scale=.25]{Images/neural_network_aplas.png}
     \end{block}}

 \uncover<2->{ \begin{block}{}
       Non-verifiable network\tikzmark{n31}
         %	\includegraphics[scale=.2]{Images/sigmoid1.png}
    %   \includegraphics[scale=.25]{Images/neural_network_aplas.png}
     \end{block}}

   
%%%%%%%%%%%%%%
     
     \column{.3\textwidth}

  %\uncover<1->{ \begin{block}{}
  %   Semantic  \tikzmark{n12} Opacity \tikzmark{n1}
    %   \includegraphics[scale=.25]{Images/neural_network_aplas.png}
  %   \end{block}}

\vspace{1cm}

     \uncover<1->{\begin{alertblock}{}
       \tikzmark{n5} can verify  \tikzmark{n23} a  \tikzmark{n7}  different object!  \tikzmark{n14} 
       \end{alertblock}}
%%%%%%%%
     \column{.3\textwidth}

   
%\vspace{0.5cm}
    
         \uncover<1->{\begin{alertblock}{}
             \tikzmark{n6}   verify a  \tikzmark{n8} smaller network
                   % \includegraphics[scale=.25]{Images/neural_network_aplas2.png}
           \end{alertblock}}
         
         \uncover<1->{\begin{alertblock}{}
             \tikzmark{n13}   verify a  \tikzmark{n8} linearised network
                  %  \includegraphics[scale=.2]{Images/sigmoid25.png}
                  \end{alertblock}}

                        \uncover<4->{\begin{alertblock}{}
             \tikzmark{n33}   re-train your network correct
                 %   \includegraphics[scale=.2]{Images/sigmoid25.png}
     \end{alertblock}}

   \end{columns}
     
   \begin{tikzpicture}[remember picture,overlay]
        %\path[draw=magenta,thick,->]<3-> ([yshift=3mm]n1) to ++(0,3mm) to [out=0, in=0,distance=2.5in] (t1);
  % \path[draw=blue, thick,->]<1-> (n1) -- (n2);
% \end{tikzpicture}
  %  \begin{tikzpicture}[remember picture,overlay]
        %\path[draw=magenta,thick,->]<3-> ([yshift=3mm]n1) to ++(0,3mm) to [out=0, in=0,distance=2.5in] (t1);
   %\path[draw=blue, thick,->]<1-> ([xshift=3mm]n3) -- (n4);
   %\path[draw=blue, thick,->]<1-3> (n12) -- (n23);
   \path[draw=blue, thick,->]<1-> (n10) -- (n5);
   \path[draw=blue, thick,->]<1-> (n7) -- (n6);
  % \path[draw=magenta,ultra thick,->]<4-> (n12) -- (n23);
   %\path[draw=magenta, ultra thick,->]<4-> (n1) -- (n5);
   %\path[draw=magenta, ultra thick,->]<4-> (n11) -- (n5);
          \path[draw=blue, thick,->]<1-> (n11) -- (n5);
          \path[draw=blue, thick,->]<1-> (n14) -- (n13);
               \path[draw=magenta, ultra thick,->]<3-> (n31) -- (n5);
       \path[draw=magenta, ultra thick,->]<4-> (n14) -- (n33);
   
\end{tikzpicture}

        \end{frame}

        \begin{frame}
\frametitle{}


\begin{center}

  
 \begin{columns}
   \column{.3\textwidth}
  \includegraphics[width=5cm]{Images/question.jpeg}
  \column{.3\textwidth}
     \column{.3\textwidth}
  \includegraphics[width=3cm]{Images/pandorra.jpeg}
\end{columns}
\end{center}
\end{frame}



\section{Continuous Verification}


\begin{frame}
  \frametitle{Continuous Verification}

 \begin{tikzpicture}[scale=.7]
 
\draw[fill=gray,draw=gray] (-2.95,.45) rectangle (1.05,-.55);  
\draw[fill=white] (-3,.5) rectangle (1,-.5); 
\node (0,0) { };
 \draw (-1,0) node {{\footnotesize \it Verifier} };

\draw[fill=gray,draw=gray] (8.95,.45) rectangle (13.30,-.55);  
\draw[fill=white] (8.9,.5) rectangle (13.25,-.5); 
\draw (11.1,0) node{{\footnotesize \it Neural Network}};


\draw (5,0.2) node{\footnotesize{\textbf{Continuous Verification}:}};
\draw (5,-0.4) node{\footnotesize{the training-verification cycle}};


\draw[latex-,shorten <=2pt,shorten >=2pt,dashed] (8.9,.4) .. controls (6,2) and (4,2) .. (1,.4); 
\draw (5,2.3) node[anchor=north,fill=white]{\emph{\footnotesize{\textcolor{red}{verify or modify constraints}}}};
 
\draw[latex-,shorten <=2pt,shorten >=2pt,dashed] (1,-.4) .. controls (4,-2) and (6,-2) .. (8.9,-.4); 
\draw (5,-2.3) node[anchor=south,fill=white]{\emph{\footnotesize{\textcolor{red}{reduce, reshape, re-train}}}}; 
\end{tikzpicture}

\pause

  \begin{block}{We have seen ``continuous verification''}
    as a trend that arises everywhere in neural network verification\\

    \begin{center}
      \alert{for a variety of different reasons!}
      \end{center}
\end{block}

\end{frame}  

\begin{frame}
  \frametitle{Continuous Verification}

 \begin{tikzpicture}[scale=.7]
 
\draw[fill=gray,draw=gray] (-2.95,.45) rectangle (1.05,-.55);  
\draw[fill=white] (-3,.5) rectangle (1,-.5); 
\node (0,0) { };
 \draw (-1,0) node {{\footnotesize \it Verifier} };

\draw[fill=gray,draw=gray] (8.95,.45) rectangle (13.30,-.55);  
\draw[fill=white] (8.9,.5) rectangle (13.25,-.5); 
\draw (11.1,0) node{{\footnotesize \it Neural Network}};


\draw (5,0.2) node{\footnotesize{\textbf{Continuous Verification}:}};
\draw (5,-0.4) node{\footnotesize{the training-verification cycle}};


\draw[latex-,shorten <=2pt,shorten >=2pt,dashed] (8.9,.4) .. controls (6,2) and (4,2) .. (1,.4); 
\draw (5,2.3) node[anchor=north,fill=white]{\emph{\footnotesize{\textcolor{red}{verify or modify constraints}}}};
 
\draw[latex-,shorten <=2pt,shorten >=2pt,dashed] (1,-.4) .. controls (4,-2) and (6,-2) .. (8.9,-.4); 
\draw (5,-2.3) node[anchor=south,fill=white]{\emph{\footnotesize{\textcolor{red}{reduce, reshape, re-train}}}}; 
\end{tikzpicture}

\pause

\begin{block}{Role of declarative programming:}
  $\epsilon$-ball verification is an instance of refinement type checking



%//val out_from_ideal': x: vector real 25{(sq_euclidean_dist #25  ideal_in  x) <. 0.01R} -> y:vector real 10{sq_euclidean_dist #10 y ideal_out <. 100.0R}
%//let out_from_ideal' x  =  run_network model x
\end{block}
  \footnotesize{
  $\mathtt{verify} \ x :\ x: \Real^n \alert{\{|\mathtt{sample\_in} - x| < \epsilon \}} \Longrightarrow y: \Real^m \alert{\{|\mathtt{sample\_out} - y| < \epsilon'\}} $\\
  $\mathtt{verify}\  x = \mathtt{run\ network}\ x $
  }

                 {\scriptsize
 \begin{thebibliography}{99}
   \beamertemplatearticlebibitems
    \bibitem{6}{Wen Kokke, E.K., Daniel Kienitz, Robert Atkey and
David Aspinall. 2020. Neural Networks, Secure by Construction: An Exploration
of Refinement Types. APLAS'20.}

\end{thebibliography}}
\end{frame}


\begin{frame}
  \frametitle{Continuous Verification}

 \begin{tikzpicture}[scale=.7]
 
\draw[fill=gray,draw=gray] (-2.95,.45) rectangle (1.05,-.55);  
\draw[fill=white] (-3,.5) rectangle (1,-.5); 
\node (0,0) { };
 \draw (-1,0) node {{\footnotesize \it Verifier} };

\draw[fill=gray,draw=gray] (8.95,.45) rectangle (13.30,-.55);  
\draw[fill=white] (8.9,.5) rectangle (13.25,-.5); 
\draw (11.1,0) node{{\footnotesize \it Neural Network}};


\draw (5,0.2) node{\footnotesize{\textbf{Continuous Verification}:}};
\draw (5,-0.4) node{\footnotesize{the training-verification cycle}};


\draw[latex-,shorten <=2pt,shorten >=2pt,dashed] (8.9,.4) .. controls (6,2) and (4,2) .. (1,.4); 
\draw (5,2.3) node[anchor=north,fill=white]{\emph{\footnotesize{\textcolor{red}{verify or modify constraints}}}};
 
\draw[latex-,shorten <=2pt,shorten >=2pt,dashed] (1,-.4) .. controls (4,-2) and (6,-2) .. (8.9,-.4); 
\draw (5,-2.3) node[anchor=south,fill=white]{\emph{\footnotesize{\textcolor{red}{reduce, reshape, re-train}}}}; 
\end{tikzpicture}

  \begin{block}{Role of logicians:}
  Solvers are increasingly important for automation
\end{block}
\pause
\begin{itemize}
  \item study tractable problems
\pause
\item devise domain-specific solvers for neural networks: 
\end{itemize}

                {\scriptsize
 \begin{thebibliography}{99}
   \beamertemplatearticlebibitems
      \bibitem{6}{Katz, G., et al.: The Marabou framework for verification and analysis of deep
neural networks. In: CAV 2019, Part I. LNCS, vol. 11561, pp. 443–452. Springer
(2019)
}
 
\end{thebibliography}}


\end{frame}




\begin{frame}
  \frametitle{Continuous Verification}

 \begin{tikzpicture}[scale=.7]
 
\draw[fill=gray,draw=gray] (-2.95,.45) rectangle (1.05,-.55);  
\draw[fill=white] (-3,.5) rectangle (1,-.5); 
\node (0,0) { };
 \draw (-1,0) node {{\footnotesize \it Verifier} };

\draw[fill=gray,draw=gray] (8.95,.45) rectangle (13.30,-.55);  
\draw[fill=white] (8.9,.5) rectangle (13.25,-.5); 
\draw (11.1,0) node{{\footnotesize \it Neural Network}};


\draw (5,0.2) node{\footnotesize{\textbf{Continuous Verification}:}};
\draw (5,-0.4) node{\footnotesize{the training-verification cycle}};


\draw[latex-,shorten <=2pt,shorten >=2pt,dashed] (8.9,.4) .. controls (6,2) and (4,2) .. (1,.4); 
\draw (5,2.3) node[anchor=north,fill=white]{\emph{\footnotesize{\textcolor{red}{verify or modify constraints}}}};
 
\draw[latex-,shorten <=2pt,shorten >=2pt,dashed] (1,-.4) .. controls (4,-2) and (6,-2) .. (8.9,-.4); 
\draw (5,-2.3) node[anchor=south,fill=white]{\emph{\footnotesize{\textcolor{red}{reduce, reshape, re-train}}}}; 
\end{tikzpicture}
  \setbeamercovered{transparent}

\begin{alertblock}{Role of declarative programming in \emph{continuous} verification?}
  \begin{itemize}[<+->]
    \item provide a \alert{sound} and \alert{elegant} PL infrastructure
    \item that bootstraps solvers and machine learning algorithms
    \item to ensure transparency, modularity,
      \item invariant and safety checks
    \end{itemize}
\end{alertblock}
%\pause
%First attempts:
%                {\scriptsize
% \begin{thebibliography}{99}
%   \beamertemplatearticlebibitems
%    \bibitem{6}{Wen Kokke, E.K., Daniel Kienitz, Robert Atkey and
%David Aspinall.  Neural Networks, Secure by Construction: An Exploration
%of Refinement Types. APLAS'20.}
%\bibitem{6}{Bagnall, A., Stewart, G.: Certifying true error: Machine learning in Coq with
%    verified generalisation guarantees. AAAI (2019).}
%\end{thebibliography}}

 \end{frame}


\begin{frame}
  \frametitle{Continuous Verification}

 \begin{tikzpicture}[scale=.7]
 
\draw[fill=gray,draw=gray] (-2.95,.45) rectangle (1.05,-.55);  
\draw[fill=white] (-3,.5) rectangle (1,-.5); 
\node (0,0) { };
 \draw (-1,0) node {{\footnotesize \it Verifier} };

\draw[fill=gray,draw=gray] (8.95,.45) rectangle (13.30,-.55);  
\draw[fill=white] (8.9,.5) rectangle (13.25,-.5); 
\draw (11.1,0) node{{\footnotesize \it Neural Network}};


\draw (5,0.2) node{\footnotesize{\textbf{Continuous Verification}:}};
\draw (5,-0.4) node{\footnotesize{the training-verification cycle}};


\draw[latex-,shorten <=2pt,shorten >=2pt,dashed] (8.9,.4) .. controls (6,2) and (4,2) .. (1,.4); 
\draw (5,2.3) node[anchor=north,fill=white]{\emph{\footnotesize{\textcolor{red}{verify or modify constraints}}}};
 
\draw[latex-,shorten <=2pt,shorten >=2pt,dashed] (1,-.4) .. controls (4,-2) and (6,-2) .. (8.9,-.4); 
\draw (5,-2.3) node[anchor=south,fill=white]{\emph{\footnotesize{\textcolor{red}{reduce, reshape, re-train}}}}; 
\end{tikzpicture}

\begin{alertblock}{Role of declarative programming in \emph{continuous} verification?}
  \setbeamercovered{transparent}
\begin{itemize}[<+->]
    \item Verification as refinement type checking
    \item Training as program synthesis
\end{itemize}
  \end{alertblock}


  
  \footnotesize{
  $\mathtt{verify} \ x :\ x: \Real^n \alert{\{|\mathtt{sample\_in} - x| < \epsilon \}} \Longrightarrow y: \Real^m \alert{\{|\mathtt{sample\_out} - y| < \epsilon'\}} $\\
  $\mathtt{verify}\  x = \mathtt{run}\ \uncover<1>{\mathtt{network}\ x} \uncover<2->{\alert{?NETWORK?}\ \ \  x}   $
  }


\end{frame}





\begin{frame}
  \frametitle{}

.

.
  \begin{block}{   Thanks for your attention!}

  \end{block}

  
\begin{center}

\includegraphics[width=4cm]{Images/question.jpeg}

\end{center}


\pause
\begin{alertblock}{PhD position at LAIV}
  ... always looking for bright students ...
  \end{alertblock}

\end{frame}

\end{document}