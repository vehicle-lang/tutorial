<!-- THIS FILE IS AUTOMATICALLY GENERATED, DO NOT EDIT IT -->

# A Vehicle Tutorial

With contributions by

- Matthew Daggitt
- Wen Kokke
- Ekaterina Komendantskaya
- Bob Atkey
- Luca Arnaboldi
- Natalia Slusarz
- Marco Casadio
- Ben Coke
- Jeonghyeon Lee

## What is Neural Network Verification about?

Neural networks are widely used in the field of machine learning; and
are often embedded as *pattern-recognition* or *signal processing*
components into complex software. Below we see a schematic depiction of
a neural network trained to classify hand-written digits: ![Neural
Network](images/mnist_classification.png)

The image is represented as a vector of real numbers, each vector
element standing for a pixel value. Each arrow in the picture bears a
*weight* that is used to multiply the input signal; each neuron computes
the sum of its inputs.

In some scenarios, it becomes important to establish formal guarantees
about neural network behaviour. Following the pioneering work of (Katz
et al. 2017, Singh2019, Wang2021) neural network verification has become
an active research area.

Formally, a neural network is a function $N : R^m \rightarrow R^n$.
Verification of such functions most commonly boils down to specifying
admissible intervals for the function’s output given an interval for its
inputs. For example, one can specify a set of inputs to belong to an
$\epsilon$- neighborhood of some given input $\mathbf{x}$, and verify
that for such inputs, the outputs of $N$ will be in $\delta$ distance to
$N(\mathbf{x})$. This property is often called $\epsilon$*-ball
robustness* (or just *robustness*), as it proves the network’s output is
robust (does not change drastically) in the neighborhood of certain
inputs.

Seen as functions, neural networks have particular features that play an
important role in their verification:

- these functions are not written manually, but generated (or *fitted*)
  to model the unknown data distribution;
- the “data” may be big, and require large neural networks;
- we often attribute very little semantic meaning to the resulting
  function.

## Challenges in Neural Network Verification

There are several research challenges in the area of neural network
verification:

1.  On the solver side, undecidability of non-linear real arithmetic
    (**Akbarpour2009?**) and scalability of neural network verifiers
    (**Wang2021?**) stand as two main challenges.
2.  In all realistic scenarious, even accurate neural networks require
    extra “property-driven training” in order to comply with
    verification properties in question. This calls for new methods of
    integrating training with verification.
3.  The scope of neural network properties available in the literature
    is limited. Robustness is the most popular general property to date
    (Casadio et al. 2021), and others include mostly domain-specific
    properties, such as ACAS Xu Benchmark (Katz et al. 2017), which we
    will consider shortly in this tutorial.
4.  The available language infrastructure (e.g. the existing neural
    network solvers) encourage property specifications in terms of the
    input space, whereas one often needs to reason about neural network
    behavior in terms of the problem space.
5.  Finally, neural networks usually work as components of complex
    systems, and the question of smooth integation of existing neural
    network solvers with other theorem provers requires resolution.

This tutorial will focus on problems 2 – 5, and will present the tool
Vehicle that provides support in alleviating them. In particular,
Vehicle provides a specification language that allows one to express
neural network properties in a high-level, human-readable format (thus
opening the way to reasoning about a wider space of properties, and
reasoning in terms of the problem space). Then it compiles the
specification down into low-level queries and passes them automatically
to existing neural network solvers. If the specification cannot be
verified, Vehicle gives one an option to automatically generate a new
loss function that can be used to train the model to satisfy the stated
property. Once a specification has been verified (possibly after
property-driven re-training), Vehicle allows one to export the proof to
an interactive theorem prover, and reason about the behavior of the
complex system that embeds the machine learning model.

Vehicle programs can be compiled to an unusually broad set of backends,
including:

1)  loss functions for Tensorflow which can be used to guide both
    specification-directed training and gradient-based counter-example
    search.

2)  queries for the Marabou neural network verifier, which can be used
    to formally prove that the network obeys the specification.

3)  Agda specifications, which are tightly coupled to the original
    network and verification result, in order to scalably and
    maintainably construct larger proofs about machine learning-enhanced
    systems.

Currently, Vehicle supports the verifier Marabou, the ITP Agda, and the
ONNX format for neural networks.

## Objectives of this Tutorial

This tutorial will give an introduction to the Vehicle tool
(<https://github.com/vehicle-lang/vehicle>) and its conceptual approach
to modelling specifications for machine learning systems via functional
programming. It will teach the participants to understand the range of
problems that arise in neural network property specification,
verification and training, and will give a hands-on experience on
solving these problems at a level of a higher-order specification
language with dependent types.

## Prerequisites

To follow the tutorial, you will need Vehicle, Marabou and Agda
installed in your machine. For instructions, refer to [vehicle
documentation](https://vehicle-lang.readthedocs.io/en/latest/installation.html).
You can also download already trained networks for our examples from
\[link to tutorial repo\].

(Recommendation to use Visual Studio Code with .vcl syntax highlighting)

## Related work

- Behzad Akbarpour and Lawrence C. Paulson. MetiTarski: An automatic
  theorem prover for real valued special functions. Journal of Automated
  Reasoning 44(3), 175–205, 2009.
- Marco Casadio, Ekaterina Komendantskaya, Matthew L. Daggitt, Wen
  Kokke, Guy Katz, Guy Amir, and Idan Refaeli. Neural network robustness
  as a verification property: A principled case study. In Computer Aided
  Verification (CAV 2022), Lecture Notes in Computer Science. Springer,
  2022.
- Guy Katz, Clarke Barrett, D. Dill, K. Julian, and M. Kochenderfer.
  Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks.
  In CAV, 2017.
- Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin T. Vechev. An
  abstract domain for certifying neural networks. Proc. ACM Program.
  Lang., 3(POPL):41:1–41:30, 2019.
- Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh,
  and J. Zico Kolter. Beta-crown: Efficient bound propagation with
  per-neuron split constraints for neu- ral network robustness
  verification. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N.
  Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances
  in Neural In- formation Processing Systems 34: Annual Conference on
  Neural Information Processing Systems 2021, NeurIPS 2021, December
  6-14, 2021, virtual, pages 29909–29921, 2021.

# Getting Started: the Vehicle’s Language

In this chapter we will introduce some basic features of **Vehicle** as
a specification language. We will use the famous *ACAS Xu verification
challenge*, first introduced in 2017 by Guy Katz et al. in *“Reluplex:
An Efficient SMT Solver for Verifying – Deep Neural Networks”
(<https://arxiv.org/pdf/1702.01135.pdf>)*

## Standard Components of a Verification Problem

In the simplest verification scenario, we will need a neural network
$N : R^m \rightarrow R^n$, and a property of the network we wish to
verify. Often, though not always, such property can be formulated based
on our understanding of the domain where the neural network is used.
ACAS Xu stands for *Airborne Collision Avoidance System for unmanned
aircraft*. The objective is to analyse the aircraft’s position and
distance relative to other aircraft and give collision avoidance
instructions.

In particular, the following measurements are of importance:

- $\rho$: feet **measuring the distance to intruder**,
- $\theta, \psi$: radians **measuring angle of intrusion**,
- $v_{own}, v_{vint}$: feet per second - **the speed of both
  aircrafts**,
- $\tau$: seconds - **time until loss of vertical separation**,
- $a_{prev}$: **previous advisory**

as the following picture illustrates: ![ACAS Xu](images/acas_xu.png)

$\theta$ and $\psi$ are measured counter clockwise, and are always in
the range $[−\pi, \pi]$.

Based on this data the neural network is to issue one of the following
instructions:

- Clear-of-Conflict (CoC),
- weak left,
- weak right,
- strong left,
- strong right.

Different neural networks are trained to analyse the relation of input
and output variables, each individual neural network uses only five
input variables. Given five selected input parameters, and the five
instructions above, a neural network $N_{AX} : R^5 \rightarrow R^5$ is
trained, given the previous historic data. The exact architecture of the
neural network, or its training mode are not important for our argument,
and so we will omit the details for now.

The original paper by Guy Katz lists ten properties, but for the sake of
the illustration we will just consider the first of them: *If the
intruder is distant and is significantly slower than the ownship, the
score of a COC advisory will always be below a certain fixed threshold.*

## Basic Building Blocks in Vehicle

### Types

Unlike many Neural Network verifiers, Vehicle is a typeful language, and
each specification file starts with declaring the types. In the ACAS Xu
case, these are

``` vehicle
type InputVector = Vector Rat 5
type OutputVector = Vector Rat 5
```

– the types of vectors of rational numbers that the network will be
taking as inputs and giving as outputs; and of course the type of the
network itself:

``` vehicle
@network
acasXu : InputVector -> OutputVector
```

The `Vector` type represents a mathematical vector, or in programming
terms can be thought of as a fixed-length array. One potentially unusual
aspect in Vehicle is that the size of the vector (i.e the number of
items it contains) must be known statically at compile time. This allows
Vehicle to check for the presence of out-of-bounds errors at compile
time rather than run time.

The full type is therefore written as `Vector A n`, which represents the
type of vectors with `n` elements of type `A`. For example,
`Vector Rat 5` is a vector of length $5$ that contains rational numbers.

**Vehicle** in fact has a comprehensive support for programming with
vectors, which we will see throughout this tutorial. But the interested
reader may go ahead and check the documentation pages for vectors:
<https://vehicle-lang.readthedocs.io/en/stable/language/vectors.html>.

Networks are declared by adding a `@network` annotation to a function
declaration, as shown above. Note that although no implementation for
the network is provided directly in the specification, the name `acasXu`
can still be used in the specification as any other declared function
would be. This follows the **Vehicle** philosophy that specifications
should be independent of any particular network, and should be able to
be used to train/test/verify a range of candidate networks
implementations.

### Values

Types for values are automatically inferred by **Vehicle**. For example,
we can declare the number $\pi$ and its type will be inferred as
rational (note the minimalistic syntax required to do that in
**Vehicle**):

``` vehicle
pi = 3.141592
```

### Working with Vectors

Often, some amount of input or output pre-processing is expected when
defining a neural network. In our case, it is assumed that the `acasXu`
neural network inputs and outputs are normalised, i.e. the network does
not work directly with units like $m/s$. However, the specifications
(and verification properties) we want to write should ideally concern
the original units.

#### Problem Space versus Input Space

When we encounter similar problems later, we will say we encountered an
instance of *problem space / input space mismatch*. These occur because
neural network models impose certain constraints on how a problem can be
expressed. In the example above, values may need to be normalised. If we
were to reason on input vectors directly, we would be writing
specifications in terms of the *input space* (i.e. referring to the
neural network inputs directly). However, when reasoning about
properties of neural networks, one often needs to refer to the original
problem. In this case specifications will be written in terms of the
*problem space*. Being able to reason about problem space (alongside the
input space) is a feature that distinguishes **Vehicle** from majority
of the mainstream neural network verifiers, such as e.g. Marabou, ERAN,
or $\alpha\beta$-Crown. Let us see how this happens in practice.

#### Vector Normalisation

We start with introducing the full block of code that normalises
vectors, and will explain significant features of Vehicle syntax
featured in the code block afterwards.

For clarity, we define a new type synonym for unnormalised input vectors
which are in the problem space.

``` vehicle
type UnnormalisedInputVector = Vector Rat 5
```

Next we define the minimum and maximum values that each input can take.
These correspond to the range of the inputs that the network is designed
to work over.

``` vehicle
minimumInputValues : UnnormalisedInputVector
minimumInputValues = [0,0,0,0,0]

maximumInputValues : UnnormalisedInputVector
maximumInputValues = [60261.0, 2*pi, 2*pi, 1100.0, 1200.0]
```

Note that above is the first instance of vector definition we encounter.
The type-checker will ensure that all vectors written in this way are of
the correct size (in this case, `5`). An alternative method to vector
definition is to use the `foreach` constructor, which is used to provide
a value for each `index i`. This method is useful if the vector has some
regular structure. In fact, the vector `minimumInputValues` could be
defined in this way:

``` vehicle
minimumInputValues : UnnormalisedInputVector
minimumInputValues = foreach i . 0
```

Let us see how `foreach` works with vector indexing. Having defined the
range of minimim and maximum values, we can define a simple predicate
saying whether a given input vector is in the right range:

``` vehicle
validInput : UnnormalisedInputVector -> Bool
validInput x = forall i . minimumInputValues ! i <= x ! i <= maximumInputValues ! i
```

Then we define the mean values that will be used to scale the inputs:

``` vehicle
meanScalingValues : UnnormalisedInputVector
meanScalingValues = [19791.091, 0.0, 0.0, 650.0, 600.0]
```

We can now define the normalisation function that takes an input vector
and returns the unnormalised version.

``` vehicle
normalise : UnnormalisedInputVector -> InputVector
normalise x = foreach i .
  (x ! i - meanScalingValues ! i) / (maximumInputValues ! i)
```

Using this we can define a new function that first normalises the input
vector and then applies the neural network:

``` vehicle
normAcasXu : UnnormalisedInputVector -> OutputVector
normAcasXu x = acasXu (normalise x)
```

### Functions

In the above block, we saw function definitions for the first time, so
let us highlight the important features of the **Vehicle** language
concerning functions.

#### Function declarations

Declarations may be used to define new functions. A declaration is of
the form

``` vehicle
<name> : <type>
<name> [<args>] = <expr>
```

Observe how all functions above fit within this declaration scheme.

#### Function types

Functions make up the backbone of the **Vehicle** language. The function
type is written `A -> B` where `A` is the input type and `B` is the
output type. For example, the function `validInput` above takes values
of the (defined) type of `UnnormalisedInputVector` and returns values of
type `Bool`. The function `normalise` has the same input type, but its
output type is `InputVector`, which was defined as a vector of rational
numbers of size $5$.

As is standard in functional languages, the function arrow associates to
the right so `A -> B -> C` is therefore equivalent to `A -> (B -> C)`.

#### Function application and composition

As in most functional languages, function application is written by
juxtaposition of the function with its arguments. For example, given a
function `f` of type `Rat -> Bool -> Rat` and arguments `x` of type
`Rat` and `y` of type `Bool`, the application of `f` to `x` and `y` is
written `f x y` and this expression has type `Bool`. This is unlike
imperative languages such as Python, C or Java where you would write
`f(x,y)`.

Functions of suitable types can be composed. For example, given a
function `acasXu` of type `InputVector -> OutputVector`, a function
`normalise` of type `UnnormalisedInputVector -> InputVector` and an
argument `x` of type `UnnormalisedInputVector` the application of
`acasXu` to the `InputVector` resulting from applying `normalise x` is
written as `acasXu (normalise x)`, and this expression has type
`OutputVector`.

#### Pre-defined functions and predicates

Some functions are pre-defined in **Vehicle**. For example, the above
block uses multiplication `*`, division `/` and vector lookup `!`. We
have also seen the use of a pre-defined “less than or equal to”
predicate `<=` in the definition of the function `validInput` (note its
`Bool` type).

## Property Definition in Vehicle

We now make up for the time invested into learning the **Vehicle**
syntax, as stating a verification property becomes very easy. As ACASXu
properties refer to certain elements of input and output vectors, let us
give those vector indices some suggestive names. This will help us to
write a more readable code:

``` vehicle
distanceToIntruder = 0   -- measured in metres
angleToIntruder    = 1   -- measured in radians
intruderHeading    = 2   -- measured in radians
speed              = 3   -- measured in metres/second
intruderSpeed      = 4   -- measured in meters/second
```

The fact that all vector types come annotated with their size means that
it is impossible to mess up indexing into vectors, e.g. if you changed
`distanceToIntruder = 0` to `distanceToIntruder = 5` the specification
would fail to type-check.

Similarly, we define meaningful names for the indices into output
vectors.

``` vehicle
clearOfConflict = 0
weakLeft        = 1
weakRight       = 2
strongLeft      = 3
strongRight     = 4
```

Let us now look at the property again:

*If the intruder is distant and is significantly slower than the
ownship, the score of a COC advisory will always be below a certain
fixed threshold.*

We first need to define what it means to be *distant and significantly
slower* The exact ACASXu definition can be written in **Vehicle** as:

``` vehicle
intruderDistantAndSlower : UnnormalisedInputVector -> Bool
intruderDistantAndSlower x =
  x ! distanceToIntruder >= 55947.691 and
  x ! speed              >= 1145      and
  x ! intruderSpeed      <= 60
```

Note the reasoning in terms of the “problem space”, i.e. the use of
unnormalised input vectors. We have already encountered the vector
lookup `!` before; but now we have a new predefined comparison function,
`>=`, “greater than or equal to”. The connective `and` is a usual
Boolean connective (note the type of the function is `Bool`).

There is little left to do, and we finish our mini-formalisation with
the property statement:

``` vehicle
@property
property1 : Bool
property1 = forall x . validInput x and intruderDistantAndSlower x =>
  normAcasXu x ! clearOfConflict <= 1500
```

To flag that this is the property we want Marabou to verify, we use the
label `@property`, we have seen this notation before when we used
`@network` to annotate the neural network declaration. The final new
bits of syntax we have not yet discussed is implication `=>` and the
quantifier `forall`.

### Quantifiers

One of the main advantages of **Vehicle** is that it can be used to
state and prove specifications that describe the network’s behaviour
over an infinite set of values. Actually, the `foreach` operator on
vectors that we have already encountered was also a quantifier – over
the finite domain of the vector indices. But `forall` is a very
different beast.

The definition of `property1` brings a new variable `x` of type
`Vector Rat 5` into scope. The variable `x` has no assigned value and
therefore represents an arbitrary input of that type. The body of the
`forall` must have type `Bool`.

Vehicle also has a matching quantifer `exists`.

## How to run **Vehicle**

To verify this property, we only need to have:

- a verifier installed (at the moment of writing Vehicle has integration
  with Marabou);
- the actual network or networks that we wish to verify. These need to
  be supplied in an ONNX format, one of the standard formats for
  representing trained neural networks.

Having these suitably installed or located, it only takes one command
line to obtain the result (note the `vcl` file, where we have written
the above specification):

``` vehicle
 vehicle \
  compileAndVerify \
  --specification acasXu.vcl \
  --verifier Marabou \
  --network acasXu:acasXu_1_7.onnx \
  --property property1
```

**Vehicle** passes the network, as well as a translation of our
specification, to Marabou, and we obtain the result – `property1` indeed
holds for the given neural network, `acasXu_1_7.onnx`:

``` vehicle
Verifying properties:
  property1 [=============================================] 1/1 queries complete
Result: true
  🗸 property1
```

## Exercises

### Exercise 1. Your first Vehicle specification

1.  On the tutorial pages, find the ONNX model, `iris_model.onnx`
    trained on the famous Iris data set:
    <https://en.wikipedia.org/wiki/Iris_flower_data_set> Find also the
    data set in the `idx` format.
2.  Using the Wikipedia page or other sources, examine the data set, and
    try to define a few “obvious properties” that should hold for a
    model that does its classification.
3.  Write those properties as a Vehicle specification, ensure it type
    checks. See Vehicl Manual pages
    (<https://vehicle-lang.readthedocs.io/en/stable/>) how to run type
    checking.
4.  Using the Vehicle command line, verify your specification,
    i.e. check whether the properties hold.

# Proving Neural Network Robustness

## Neural Network Robustness as a Verification Property

In this chapter we will learn about the problem that has received
significanrt attention within the machine learning community: *the
problem of robustness of neural networks to out-of-distribution shifts,
also known as “robustness to adversarial attacks”.* The problem was
famously raised by Christian Szegedy and his co-authors in 2013 in the
paper [“Intriguing properties of neural
networks”](https://arxiv.org/pdf/1312.6199.pdf)

So, here is the problem. Suppose we are given a data set $\mathcal{X}$
for classification of images, it consists of pairs
$(\mathbf{x}, \mathbf{y})$, where $\mathbf{x} \in \mathbb{R}^n$ is an
input, and $\mathbf{y} \in \mathbb{R}^m$ is the desired output. It is
assumed that the outputs $\mathbf{y}$ are generated from $\mathbf{x}$ by
some function $\mathcal{H} : \mathbb{R}^n → \mathbb{R}^m$ and that
$\mathbf{x}$ is drawn from some probability distribution over
$\mathbb{R}^n$.

Let us take as an example the famous [MNIST data set by LeCun et
al.](https://www.tensorflow.org/datasets/catalog/mnist) The images look
like this:

<figure>
<img src="images/MNIST.jpeg" alt="Images from the MNIST data set" />
<figcaption aria-hidden="true">Images from the MNIST data
set</figcaption>
</figure>

As we have already discussed in the previous chapter, a neural network
is a function $f : \mathbb{R}^n → \mathbb{R}^m$ parametrised by a set of
weights $\mathbf{w}$. The goal of training is to use the dataset
$\mathcal{X}$ to find weights $\mathbf{w}$ such that $f$ approximates
$\mathcal{H}$ well over input regions with high probability density.

When we train a neural network to be highly accurate on both the
training and the test sets, we emprically test:

- how well the neural network can in principle approximate $\mathcal{H}$
  (we do this by measuring its accuracy on the training set);
- how well that learnt hypothesis generalises to yet unseen data (we do
  this by measuring the accuracy on the test set).

Coming to our example, if my neural network has a $99$ % accuracy on the
MNIST data set, I should be satisfied that it learnt what a hand-written
digit is. Szegedy et al were the first to show systematically that this
is not the case: take the image on the left (below), which is classified
with high confidence as “0”, apply perturbation on the middle to get the
image on the right, and your neural network will give a $94$ %
confidence that it sees a “5” on the right, even despite the fact that
the image did not change the class (for the human eye):

|           Original MNIST Image           |          Perturbation           | Resulting Perturbed Image          |
|:----------------------------------------:|:-------------------------------:|:-----------------------------------|
| ![Original MNIST Image](images/true.png) | ![Perturbation](images/eta.png) | ![Perturbed Image](images/adv.png) |

This experiment can be replicated for any data set and any neural
network, no matter how accurate it is.

The root of the problem is: the image on the right no longer belongs to
the probability distribution that the network has learnt (whether or not
the image looks the same to a human observer). We could phrase it
differently: an ideal probability distribiution $\mathcal{H}$ that is
“learnt” by a “human” accounts not only for the images we obtained as
part of the dataset $\mathcal{X}$, but also involves an implicit
assumption that “semantically” similar images belong to the same class.

The simplest way to capture this implicit assumption is to formulate a
*verification property* that insists that all similar images (images
within an $\epsilon$ distance of each other in $\mathbb{R}^n$) are
classified similarly. This property is often called $\epsilon$-ball
robusness. For every image in the dataset, we assume we can “draw” a
small $\epsilon$-ball around it, and guarantee that within that
$\epsilon$-ball classification of the network does not change much (the
ball’s radius below is given by the chosen $\epsilon$):

|     $\epsilon$-ball around a number “7” in MNIST     |
|:----------------------------------------------------:|
| ![epsilon-ball](images/neighbourhood-robustness.png) |

Formally, we define an $\epsilon$-ball around an image
$\hat{\mathbf{x}}$ as:

$$\mathbb{B}(\hat{\mathbf{x}}, \epsilon) = [  \mathbf{x} \in \mathbb{R}^n: |\hat{\mathbf{x}}-\mathbf{x}| \leq \epsilon ]$$

where $| ... |$ is a distance function (or $L$-norm) in $\mathbb{R}^n$,
such as Euclidean distance or $L_{\infty}$-norm.

It now remains to define the property “classification of $f$ does not
change much”. The paper by [Casadio et
al.](https://arxiv.org/abs/2104.01396) summarises a few options for this
definition. The simplest is the *Classification Robustness* that
requires that all images within any given $\epsilon$-ball are classified
as the same class. We will consider this property in detail, and will
take a few other properties from Casadio et al. as an exercise.

## Formalising $\epsilon$-ball robustness for MNIST networks in Vehicle

We note that $\epsilon$-ball robustness as a verification property bears
some similarity to the ACAS Xu example that we have already covered in
Chapter 1. In particular, both verification properties impose
constraints on the output regions of the neural networks, assuming some
constraint on their input regions. (Both problems are therefore amenable
to a range of interval propagation and abstract interpretation methods,
see [this survey](https://arxiv.org/abs/1812.08342) for further
details.) From the point of view of the property specification, which is
our main concern here, there are three main differences between these
two examples:

- ACAS Xu did not have to refer to a dataset $\mathcal{X}$;
  $\epsilon$-ball robustness, however, is formulated relative to the
  images given in the data set. We will see how Vehicle can be used to
  handle properties that refer directly to the data sets.

- MNIST, as many other data sets used in Computer Vision, has images
  represented as 2D arrays. Such data sets often require Convolutional
  Neural Networks (NN) that are best desgned to deal with 2D and 3D
  data. In terms of property specification, we will see Vehicle’s
  support for 2D arrays, which comes for free with its general type
  infrastructure.

- Finally, the MNIST specification involves two parameters that we may
  want to pass or infer at the compilation time rather than hard-code
  within the spec. These are the $\epsilon$ and the number of data
  points ($\epsilon$-balls) we wish to check (the number is at most the
  size of the entire data set). We will see how such parameters are
  defined and used in Vehicle.

### 2D Arrays in Vehicle

Starting a specification for MNIST data set follows the same pattern as
we have seen in Chapter 1, only this time we declare inputs as 2D
arrays:

``` vehicle
type Image = Tensor Rat [28, 28]
type Label = Index 10
```

As before, we define valid inputs, this time making a mild adaptation to
2D arrays and assuming all pixel values are normalised between 0 and 1:

``` vehicle
validImage : Image -> Bool
validImage x = forall i j . 0 <= x ! i ! j <= 1
```

The output of the network is a score for each of the digits 0 to 9.

``` vehicle
@network
classifier : Image -> Vector Rat 10
```

We note again the use of the syntax for `@network`, marking the place
where Vehicle interacts with an external tool (this time most likely
with Python Tensorflow).

The classifier advises that input image `x` has label `i` if the score
for label `i` is greater than the score of any other label `j`:

``` vehicle
advises : Image -> Label -> Bool
advises x i = forall j . j != i => classifier x ! i > classifier x ! j
```

This completes the basic description if the data set and the model
architecture in Vehicle. We are ready to define verification properties.

### Definition of Robustness Around a Point

First we define the parameter `epsilon` that will represent the radius
of the balls that we verify. Note that we declare this as a parameter
which allows the value of `epsilon` to be specified at compile time
rather than be fixed in the specification. We again use the syntax `@`
to communicate this information externally:

``` vehicle
@parameter
epsilon : Rat
```

Next we define what it means for an image `x` to be in a ball of size
epsilon. The definition below uses the $L_{\infty}$ norm, defined as:

$$|\mathbf{x}|_{\infty} = max (\mathbf{x})$$

where $max (\mathbf{x})$ computes the maximum element of $\mathbf{x}$.
Below, we shortcut a bit the calculation of $|\mathbf{x}|_{\infty}$
being bounded by $\epsilon$ and simply require that all vector elements
are bounded by $\epsilon$:

``` vehicle
boundedByEpsilon : Image -> Bool
boundedByEpsilon x = forall i j . -epsilon <= x ! i ! j <= epsilon
```

Using the Eucledian distance would require a slightly more complicated
definition, which we will do as an exercise.

We now define what it means for the network to be robust around an image
`x` that should be classified as `y`. Namely, we define that for any
perturbation no greater than $\epsilon$, if the perturbed image is still
a valid image then the network should still advise label `y` for the
perturbed version of `x`.

``` vehicle
robustAround : Image -> Label -> Bool
robustAround image label = forall pertubation .
  let perturbedImage = image - pertubation in
  boundedByEpsilon pertubation and validImage perturbedImage =>
    advises perturbedImage label
```

Again, note the use of a quantifier `forall` that ranges over an
infinite domain of images of type `Image`.

## Definition of Robustness with Respect to a Dataset

We first specify parameter `n` , which stands for the size of the
training dataset. Unlike the earlier parameter `epsilon`, we set the
`infer` option of the parameter `n` to ‘True’. This means that it does
not need to be provided manually but instead will be automatically
inferred by the compiler. In this case it will be inferred from the
training datasets.

``` vehicle
@parameter(infer=True)
n : Nat
```

We next declare two datasets, the training images and the corresponding
training labels. Note that we use the previously declared parameter `n`
to enforce that they are of the same size:

``` vehicle
@dataset
trainingImages : Vector Image n

@dataset
trainingLabels : Vector Label n
```

Again we note the use of syntax that involves `@` flagging Vehicle’s
connection with an external tool or object – in this case, the data set
is defined externally.

We then say that the network is robust *for this data set* if it is
robust around every pair of input images and output labels. Note once
again the use of the `foreach` keyword when quantifying over the index
`i` in the dataset. Whereas `forall` would return a single `Bool`,
`foreach` constructs a `Vector` of booleans, ensuring that Vehicle will
report on the verification status of each image in the dataset
separately. If `forall` was omitted, Vehicle would only report if the
network was robust around *every* image in the dataset, a state of
affairs which is unlikely to be true.

``` vehicle
@property
robust : Vector Bool n
robust = foreach i . robustAround (trainingImages ! i) (trainingLabels ! i)
```

## Running the Verification Query

In order to run Vehicle, we need to provide:

- the specification file,
- the network in ONNX format,
- the data in idx format,
- and the desired $\epsilon$ value.

The tutorial files contain two Python scripts that show how to convert
Tensorflow Neural Networks into *ONNX* format; and images – into `.idx`
files. These are the formats expected by Vehicle. You can use the ones
we provide, or generate your own. Having obtained these, the following
command line will take care of verification of the network
`mnist-classifier.onnx`, for data sets `images.idx` and `labels.idx` and
$\epsilon = 0.005$:

``` vehicle
vehicle verify \
  --specification examples/mnist-robustness/mnist-robustness.vcl \
  --network classifier:examples/mnist-robustness/mnist-classifier.onnx \
  --parameter epsilon:0.005 \
  --dataset trainingImages:examples/mnist-robustness/images.idx \
  --dataset trainingLabels:examples/mnist-robustness/labels.idx \
  --verifier Marabou
```

For the first two images in your data set, the output will look as
follows:

``` vehicle
Verifying properties:
  robust [================================================] 9/9 queries complete
  robust [================================================] 9/9 queries complete
Result: true
  robust: 2/2 verified
    🗸 robust!0

    🗸 robust!1
```

The reader may have guessed at this pont that, as we make $\epsilon$
larger, fewer and fewer examples will staisfy the property. Chapter 3
will look into methods that can be used to train networks to satisfy
robustness for larger $\epsilon$.

## Exercises

### Exercise 1: Standard Robustness in Vehicle

Define and verify in Vehicle the propety of *Standard Robustness*, that
requires, for all $\mathbf{x}$ in the $\epsilon$-ball of
$\hat{\mathbf{x}}$, that
$|f(\hat{\mathbf{x}}) - f(\mathbf{x})| \leq \delta$, for some small
$\delta$. We now assemble the desired *standard robustness* property
definition:

Given an $\hat{\mathbf{x}} \in \mathcal{X}$,

\$. \|-\| \|f() - f()\| \$.

We refer the interested reader for a more detailed discussion of
different robustness properties in:

- Marco Casadio, Ekaterina Komendantskaya, Matthew L. Daggitt, Wen
  Kokke, Guy Katz, Guy Amir, Idan Refaeli: Neural Network Robustness as
  a Verification Property: A Principled Case Study. CAV (1) 2022:
  219-231.

- Xiaowei Huang, Daniel Kroening, Wenjie Ruan, James Sharp, Youcheng
  Sun, Emese Thamo, Min Wu, Xinping Yi. A Survey of Safety and
  Trustworthiness of Deep Neural Networks: Verification, Testing,
  Adversarial Attack and Defence, and Interpretability. J. of Computer
  Science Review, 2018.

### Exercise 2: Explore Other Definitions of Robustness

Use Vehicle to define other forms of Robustness property from Casadio et
al.

### Exercise 3: Other Distances in Vehicle

Re-define the *classification* and *standard robustness* properties by
using some different notion of distance, e.g. the Euclidean distance,
instead of the $L_{\infty}$ norm.

### Exercise 4: Conduct a complete “training - verification” experiment from start to finish

Download the [Fashion MNIST data
set](https://www.tensorflow.org/datasets/catalog/fashion_mnist), train
the model, generate `onnx` and `idx` files, define the spec and verify
its robustness.

# Property-Driven Training

## Motivation and Problem Statement

We finished the last chapter with a conjecture concerning diminishing
robustness verification success with increasing values of $\epsilon$.
Let us now see, using a concrete example, how soon the success rate
declines.

The last exercise of the previous chapter gave us a property
specification for robustness of \`\`Fashion MNIST” models. We propose
now to look into the statistics of verifying one of such models on 500
examples from the data set. To obtain quicker verification times, let us
use a Fashion MNIST model with one input layer of $32$ neurons, and one
output layer of $10$ neurons (the tutorial files contain the model if
you wish to check it). Running Vehicle, we obtain the following success
rates:

| $\epsilon = 0.01$ | $\epsilon = 0.05$ | $\epsilon = 0.1$ | $\epsilon = 0.5$ |
|:-----------------:|:-----------------:|:----------------:|:-----------------|
| 82.6 % (413/500)  | 29.8 % (149/500)  |  3.8 % (19/500)  | 0 % (0/500)      |

As we see in the table, verifiability of the property deteriorates
quickly with growing $\epsilon$. Yet, for majority of practical
applications, it is desirable to have a larger $\epsilon$, as this
increases the chance that new yet unseen data points will fall within
the verified subspaces of the input vector space.

Can we re-train the neural network to be more robust within a desirable
$\epsilon$? The long tradition of robustifying neural networks in
machine learning has a few methods ready, for example, to re-train the
networks with new data set that was augmented with images within the
desired $\epsilon$-balls, or to generate adversarial examples (sample
images closest to the decision boundary) within the given
$\epsilon$-balls. We once again refer the reader to

- Marco Casadio, Ekaterina Komendantskaya, Matthew L. Daggitt, Wen
  Kokke, Guy Katz, Guy Amir, Idan Refaeli: Neural Network Robustness as
  a Verification Property: A Principled Case Study. CAV (1) 2022:
  219-231.

for further discussion of these various methods.

In this tutorial, however, our interest is in *specification-driven*
neural network verification. Our interest is thus in generating suitable
loss functions directly from specifications. Crucially, this will allow
us to work with arbitrary properties of neural networks, not only
robustness.

Traditionally, translations from a given logical syntax to a loss
function are known as “differentiable logics”, or DLs. One of the first
attempts to translate propositional logic specifications to loss
functions was given in (**Xu?** et al. 2018):

- Jingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, and Guy Van den
  Broeck. 2018. A Semantic Loss Function for Deep Learning with Symbolic
  Knowledge. In Proceedings of the 35th International Conference on
  Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July
  10-15, 2018 (Proceedings of Machine Learning Research, Vol. 80),
  Jennifer G. Dy and Andreas Krause (Eds.). PMLR, 5498–5507.
  <http://proceedings.mlr.press/v80/xu18h.html>

and was generalised to a fragment of first-order logic in (**Fischer?**
et al. 2019):

- Marc Fischer, Mislav Balunovic, Dana Drachsler-Cohen, Timon Gehr, Ce
  Zhang, and Martin T. Vechev. 2019. DL2: Training and Querying Neural
  Networks with Logic. In Proceedings of the 36th International
  Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach,
  California, USA (Proceedings of Machine Learning Research, Vol. 97),
  Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, 1931–1941.
  <http://proceedings.mlr.press/v97/fischer19a.html>

Later, this work was complemented by giving a fuzzy interpretation to DL
by (**van?** Krieken et al. 2022):

- Emile van Krieken, Erman Acar, and Frank van Harmelen. 2022. Analyzing
  Differentiable Fuzzy Logic Operators. Artif. Intell. 302
  (2022), 103602. <https://doi.org/10.1016/j.artint.2021.103602>

Slusarz et al. \[2023\] proposed generalisation for the syntax and
semantics of DL, with a view of encoding all previously presented DLs in
one formal system, and comparing their theoretical properties:

- Natalia Slusarz, Ekaterina Komendantskaya, Matthew L. Daggitt,
  Robert J. Stewart, and Kathrin Stark. 2023. Logic of Differentiable
  Logics: Towards a Uniform Semantics of DL. In LPAR-24: The
  International Conference on Logic for Programming, Artificial
  Intelligence and Reasoning.

Following this work, Vehicle contains translation to several loss
functions available in the literature.

# References

<div id="refs" class="references csl-bib-body hanging-indent">

<div id="ref-CasadioDKKKS2022" class="csl-entry">

Casadio, Marco, Matthew L. Daggitt, Ekaterina Komendantskaya, Wen Kokke,
Daniel Kienitz, and Rob Stewart. 2021. “Property-Driven Training: All
You (n)ever Wanted to Know About.” *CoRR* abs/2104.01396.
<https://arxiv.org/abs/2104.01396>.

</div>

<div id="ref-KatzBDJK2017" class="csl-entry">

Katz, Guy, Clark W. Barrett, David L. Dill, Kyle Julian, and Mykel J.
Kochenderfer. 2017. “Reluplex: An Efficient SMT Solver for Verifying
Deep Neural Networks.” *CoRR* abs/1702.01135.
<http://arxiv.org/abs/1702.01135>.

</div>

</div>
