# Property-Driven Training

## Motivation

We finished the last chapter with a conjecture concerning
diminishing robustness verification success with increasing values of $\epsilon$.
Let us now see, using a concrete example, how soon the success rate declines.

The last exercise of the previous chapter gave us a property specification
for robustness of ``Fashion MNIST" models. We propose now to look into the statistics of verifying one of such models on 500 examples from the data set. To obtain quicker verification times, let us use a Fashion MNIST model with one input layer of $32$ neurons, and one output layer of $10$ neurons (the tutorial files contain the model if you wish to check it). Running Vehicle, we obtain the following success rates:

| $\epsilon = 0.01$ | $\epsilon = 0.05$ | $\epsilon = 0.1$ | $\epsilon = 0.5$ |
| :---------------: | :---------------: | :--------------: | :--------------- |
| 82.6 % (413/500)  | 29.8 % (149/500)  |  3.8 % (19/500)  | 0 % (0/500)      |

As we see in the table, verifiability of the property deteriorates quickly with growing
$\epsilon$. Yet, for majority of practical applications, it is desirable to have a larger $\epsilon$,
as this increases the chance that new yet unseen data points will fall within the verified
subspaces of the input vector space.

Can we re-train the neural network to be more robust within a desirable $\epsilon$?
The long tradition of robustifying neural networks in machine learning has a few methods
ready, for example, to re-train the networks with new data set that was augmented with images within the
desired $\epsilon$-balls, or to generate adversarial examples (sample images closest to the decision boundary) within the given $\epsilon$-balls during training. 

Let us look closer into this.

## Robustness Training

### Data Augmentation

 Suppose we are given a data set $\mathcal{D} =  \{(\x_1, \y_1), \ldots , (\x_n, \y_n)\}$. \\
  Prior to training, we can generate new training data samples within $\epsilon$-balls of the existing data and label them with the same output as the original data. Then we can use our usual training methods with this new *augmented data set* ([@SK19]).

However, this method maybe problematic for verification purposes.
Let us have a look at its effect, pictorially. Suppose this is the manifold that corresponds to $\mathcal{D}$ (crosses are the original data points, and circles are the $\epsilon$-balls around them):

![Data Manifold for D](images/SR-vs-CR-2.png)

Remember that we sampled our new data from these $\epsilon$-balls.
But suppose your true decision boundary runs over the manifold like this:

![Data Manifold for D](images/SR-vs-CR-4.png)

We have a problem, because some of the data points we sampled from the  suddenly have wrong labels!

Actually, it maybe even worse. Depending how our data lies on the manifold, we may have even generated inconistent labelling. Here is the example when this happens:

![Data Manifold for D](images/SR-vs-CR-5.png)

It seems data augmentation is not general enough for *Vehicle*, and only works correctly if strong assumptions about the underlying manifold are taken.

### Adversarial Training

### Further reading
Some of these problems are discussed more formally in

- Marco Casadio, Ekaterina Komendantskaya, Matthew L. Daggitt, Wen Kokke, Guy Katz, Guy Amir, Idan Refaeli: Neural Network Robustness as a Verification Property: A Principled Case Study. CAV (1) 2022: 219-231.


However, many of these methods are specific only to robustness specifications.
As a baseline we would like a method that works for any Vehicle specification.
_Logical loss functions_ are one such method.

## Logical loss functions

The main idea is that we would like to co-opt the same gradient-descent algorithm that is used to
train the network to fit the data to also train the network to obey the specification.

Consider the very simple example specification:

```vehicle
@network
f : Vector Rat 1 -> Vector Rat 1

@property
greaterThan2 : Bool
greaterThan2 = f [ 0 ] ! 0 > 2
```

This statement is either true or false, as shown in the left graph below:

![Boolean loss](images/boolean-loss.png)

However, what if instead, we converted all `Bool` values to `Rat`, where a value greater than
`0` indicated false and a value less than `0` indicated true?
We could then rewrite the specification as:

```vehicle
greaterThan2 : Rat
greaterThan2 = f [ 0 ] ! 0 - 2
```

If we then replot the graph we get the following:

![Rational loss](images/real-loss.png)

Now we have a useful gradient, as successfully minimising `f [ 0 ] ! 0 - 2` will result in the property `greaterThan2` becoming true.

This is the essence of logical loss functions: convert all booleans and operations over booleans
into equivalent numeric operations that are differentiable and whose gradient's point in the
right direction.

Traditionally, translations from a given logical syntax to a loss function are
known as â€œdifferentiable logics", or DLs.
One of the first attempts to translate propositional logic specifications to loss functions was given in [[@XuZFLB18]](http://proceedings.mlr.press/v80/xu18h.html) and was generalised to a fragment of first-order logic in [[@FischerBDGZV19]](http://proceedings.mlr.press/v97/fischer19a.html).
Later, this work was complemented by giving a fuzzy interpretation to DL by [[@KriekenAH22]](https://doi.org/10.1016/j.artint.2021.103602) and [[@SlusarzKDSS23]](https://arxiv.org/abs/2303.10650) proposed generalisation for the
syntax and semantics of DL, with a view of encoding all previously presented DLs in one formal
system, and comparing their theoretical properties.

Vehicle has several different differentiable logics from the literature available, but will not go into detail about
how they work here.

## Generating a logical loss functions for mnist-robustness

To generate a logical loss function usable in Python, we will use Vehicle's Python bindings that come pre-installed.

We open a new Python file and write:

```python
from vehicle_lang.compile import Target, to_python

spec = to_python(
    "mnist-robustness.vcl",
    target=Target.LOSS_DL2,
    samplers={"pertubation": sampler_for_pertubation},
)

robust_loss_fn = spec["robust"]
```

which generates the loss function representing the logical loss function for the property `robust`.

This can then be used in a standard training loop:

```python
model = tf.Sequential([tf.Input(shape=(28,28)), tf.Dense(units=28), tf.Output(units=10)])

for epoch in range(num_epochs):
    for x_batch_train, y_batch_train in train_dataset:
        with tf.GradientTape() as tape:
            # Calculate standard cross entropy loss
            outputs = model(x_batch_train, training=True)
            ce_loss_value = ce_batch_loss(y_batch_train, outputs)

            # Calculate the robustness loss
            robust_loss_value = robust_loss_fn(
                n=len(x_batch_train),
                classifier=model,
                epsilon=0.001,
                trainingImages=x_batch_train,
                trainingLabels=y_batch_train,
            )
            weighted_loss = 0.5 * ce_loss_value + 0.5 * robust_loss_value

        grads = tape.gradient(weighted_loss, model.trainable_weights)
        optimizer.apply_gradients(zip(grads, model.trainable_weights))
```
