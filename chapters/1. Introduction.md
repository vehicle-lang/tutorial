# Introduction

Welcome to the *Vehicle* tutorial page!
In this tutorial, you will learn about neural network verification with *Vehicle*. *Vehicle*'s complete documentation and installation instructions are available on the [vehicle documentation](https://vehicle-lang.readthedocs.io/en/latest/installation.html) page.

The present tutorial contains contributions by:

- Matthew Daggitt
- Wen Kokke
- Ekaterina Komendantskaya
- Bob Atkey
- Luca Arnaboldi
- Natalia Slusarz
- Marco Casadio
- Ben Coke
- Jeonghyeon Lee

This tutorial has been run live and adapted for different audiences at:

- [FOMLAS'23 the 6th Workshop on Formal Methods for ML-Enabled Autonomous Systems](https://fomlas2023.wixsite.com/fomlas2023) co-allocated with CAV'23 in Paris, France; 18 July 2023;

- [VETSS Summer School](https://wp.doc.ic.ac.uk/vss/) at the University of Surrey, UK; 23 August 2023;

- [ICFP'23 the International Conference on Functional Programming](https://icfp23.sigplan.org/) in Seattle, USA; 08 September 2023.

We thank all our readers and attendees for their feedback and contribution. If you have any questions about the *Vehicle* tutorial, you are invited to join [this Slack Channel](https://join.slack.com/t/newworkspace-q484682/shared_invite/zt-1zhh8ql35-arcDaG0961BsI1nCcOWyEQ) for questions and discussions.

_We hope you enjoy this course!_

## What is Neural Network Verification?

Neural networks are widely used in the field of machine learning; and are often embedded as _pattern-recognition_ or _signal processing_ components into complex software. Below we see a schematic depiction of a neural network trained to classify hand-written digits:
![Neural Network](images/mnist_classification.png)

The image is represented as a vector of real numbers, each vector element standing for a pixel value. Each arrow in the picture bears a _weight_ that is used to multiply the input signal; each neuron computes the sum of its inputs.

In some scenarios, it becomes important to establish formal guarantees about neural network behaviour. One of the first known attempts to verify neural networks, by [@PT10], was based on abstract interpretation.
The famous paper by Szegedy [@szegedy2013intriguing] that highlighted the problem of neural network vulnerability to small-norm input perturbations ("adversarial attacks") gave additional impulse to this line of research. In CAV'2017, two papers, by Huang et al [@HuangKWW17] and
Katz et al. [@katz2017reluplex], on neural network verification appeared and both used specialised forms of SMT-solving. The later gave rise
to Marabou [@katz2019marabou], -- a rapidly developing sound and complete neural network verifer, which we use in *Vehicle*.

In 2019, the ERAN verifier by Dingh et al. [@singh2019abstract] appeared in POPL, and showed that performance of abstract interpretation methods
compares favourably against those based on SMT solving. However, the range of properties they handled was limited to proving adversarial robustness of neural networks; moreover ERAN was incomplete.
This line of research led to many subsequent extensions, e.g. by Muller et al. [@muller2022prima], [@mueller2023abstract] to mention a few.

Mixed Integer Linear Programming (MILP) methods were brought into this community by Bastani et al. [@BastaniILVNC16], and were further developed into working tools, e.g. Venus [@BotoevaKKLM20].

Neural network verifier extensions followed two main directions:

- scaling to larger networks (we can mention $\alpha\beta$-Crown [@wang2021beta] and GCP-Crown [@zhang2022general] as VNN-COMP winners in 2021 and 2022);

and

- extending from piece-wise linear to non-linear activation functions. (An example are sigmoid neurons handled by Verisig using interval arithmetic [@IvanovWAPL19].)

At the time of writing, there exist over a hundred verifiers for neural networks. Several papers and monographs are dedicated to the survey of the landscape [@LiuALSBK21,@PGL-051,@HuangKRSSTWY20]. The community established the specification standards [VNNLib](https://www.vnnlib.org/), common benchmarks and annual competitions. *Vehicle* compiles down to the VNNLib standard, with a view to be compatible with the growing family of verifiers.

Formally, a neural network is a function $N : R^m \rightarrow R^n$. Verification of such functions most commonly boils down to specifying admissible intervals for the function's output given an interval for its inputs. For example, one can specify a set of inputs to belong to an $\epsilon$- neighborhood of some given input $\mathbf{x}$, and verify that for such inputs, the outputs of $N$ will be in $\delta$ distance to $N(\mathbf{x})$. This property is often called $\epsilon$_-ball robustness_ (or just _robustness_), as it proves the network's output is robust (does not change drastically) in the neighborhood of certain inputs.

Seen as functions, neural networks have particular features that play an important role in their verification:

- these functions are not written manually, but generated (or _fitted_) to model the unknown data distribution;
- the "data" may be big, and require large neural networks;
- we often attribute very little semantic meaning to the resulting function.

## Challenges in Neural Network Verification

There are several research challenges in the area of neural network verification:

1. _Theory: finding appropriate verification properties._ The scope of neural network properties available in the literature is limited. Robustness is the most popular general property to date [@casadio2021property], and others include mostly domain-specific properties, such as ACAS Xu Benchmark [@katz2017reluplex], which we will consider shortly in this tutorial. What neural network properties we want and can realistically verify still stands as a big research question.

2. _Solvers: undecidability of non-linear real arithmetic and scalability of neural network verifiers._ On the solver side, undecidability of non-linear real arithmetic [@Akbarpour2009] and scalability of neural network verifiers [wang2021beta] stand as two main challenges.

3. _Machine Learning: understanding and integrating property-driven training._ In all realistic scenarios, even accurate neural networks require extra "property-driven training" in order to comply with verification properties in question. This requires new methods of integrating training with verification. Several approaches exist, including the recently introduced method of _"differentiable logics"_ that translate logical properties into loss functions. But they all have pitfalls, see [@SKDSS23] for a discussion.

4. _Programming: finding the right languages to support these developments_ Many existing solvers have low-level syntax that is hard to understand, making maintenance of the specification difficult. There is very little programming infrastructure to interface verification and property-driven training. Furthermore, the available language infrastructure only allows specifications to be written in terms of the input space, whereas one often needs to reason about neural network behavior in terms of the problem space. This creates an additional _embedding gap_ on verification side, -- a problem that eventually needs to be resolved.

5. _Complex systems: integration of neural net verification into complex systems._ Finally, neural networks usually work as components of complex systems, and the question of smooth integation of existing neural network solvers with other theorem provers requires resolution.

This tutorial will focus on problems 3 -- 5, and will present *Vehicle*, a tool that provides support in alleviating them. In particular, *Vehicle* is equipped with a specification language that allows one to express neural network properties in a high-level, human-readable format (thus opening the way to reasoning about a wider space of properties, and reasoning in terms of the problem space). Then it compiles the specification down into low-level queries and passes them automatically to existing neural network solvers. If the specification cannot be verified, *Vehicle* gives one an option to automatically generate a new loss function that can be used to train the model to satisfy the stated property. Once a specification has been verified (possibly after property-driven re-training), *Vehicle* allows one to export the proof to an interactive theorem prover, and reason about the behavior of the complex system that embeds the machine learning model.

*Vehicle* programs can be compiled to an unusually broad set of backends,
including:

a) loss functions for Tensorflow which can be used to guide
both specification-directed training and gradient-based counter-example
search.

b) queries for the Marabou neural network verifier, which
can be used to formally prove that the network obeys the specification.

c) Agda specifications, which are tightly coupled to the original network
and verification result, in order to scalably and maintainably construct
larger proofs about machine learning-enhanced systems.

Currently, *Vehicle* supports the verifier Marabou, the ITP Agda, and the ONNX format for neural networks.
The below figure illustrates the existing user backends in *Vehicle*.

![Vehicle Backends](images/vehicle-structure.png)


## Objectives of this Tutorial

This tutorial will give an introduction to the *Vehicle* tool
(<https://github.com/vehicle-lang/vehicle>) and its conceptual approach
to modelling specifications for machine learning systems via functional
programming. It will teach the participants to understand the
range of problems that arise in neural network property specification,
verification and training, and will give hands-on experience at
solving these problems at a level of a higher-order specification
language with dependent types.

## Prerequisites

To follow the tutorial, you will need *Vehicle*, Marabou and Agda installed in your machine.
For instructions, refer to [vehicle documentation](https://vehicle-lang.readthedocs.io/en/latest/installation.html).

Whether you are using this tutorial for self-study or attending one of our live tutorials, all supporting exercises, code and infrastructure can be downloaded from the [tutorial repository](https://github.com/vehicle-lang/vehicle-tutorial). These include all relevant  property specifications, trained neural networks in ONNX format, data in IDX format, and any necessary instructions. 

We recommend using [Visual Studio Code](https://code.visualstudio.com) with the [Vehicle Syntax Highlighting](https://marketplace.visualstudio.com/items?itemName=wenkokke.vehicle-syntax-highlighting) and [agda-mode](https://marketplace.visualstudio.com/items?itemName=banacorn.agda-mode) plugins.
